This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-12T13:13:29.046Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.openhands/
  microagents/
    repo.md
.github/
  workflows/
    check.yml
    pr-auto-approve.yaml
  CODEOWNERS
  release.yml
app/
  api/
    v1/
      __init__.py
      agents.py
      chat.py
      line.py
      schedule.py
      web_socket.py
    __init__.py
  auth/
    __init__.py
    api_key.py
  core/
    __init__.py
    agent_manager.py
    config.py
    llm_service.py
    memory_service.py
    schedule_service.py
    service_factory.py
    stt_service.py
    tts_service.py
  schemas/
    __init__.py
    agent.py
    chat.py
    emotion.py
    llm.py
    schedule.py
    status.py
    web_socket.py
  utils/
    audio.py
  __init__.py
  main.py
docs/
  design.md
.gitignore
compose-dev.yml
compose.yml
Dockerfile
Dockerfile-dev
example.env
LICENSE
LICENSE_JP
pyrightconfig.json
README_JP.md
README.md
requirements.txt
start.sh
uvicorn_log_config.yaml

================================================================
Files
================================================================

================
File:  .openhands/microagents/repo.md
================
Repository: Karakuri Agent
Description: Karakuri Agent is an open-source project aiming to create an AI agent accessible from any environment—whether it's a smart speaker, chat tool, or web application. By integrating various endpoints and services, it aims to realize a world where you can access a single agent from anywhere.
You can also define multiple agents simultaneously, each with unique roles, personalities, voices, and models.

Directory Structure:
- app/: Main application code

Setup:
- Run `apt update && apt install -y build-essential libsndf` and `pip install -r requirements.txt` to install dependencies
- Use `./start.sh` for run server

Guidelines:
- Run `ruff check` and `ruff format` and `pyright` after modifying code
- This project is powered by FastAPI.
- Use python for new code

================
File: .github/workflows/check.yml
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

name: check

on:
  pull_request:
  workflow_dispatch:

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  analyze-server:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run pyright on server
        run: pyright
      - name: Run Ruff check
        run: ruff check

================
File: .github/workflows/pr-auto-approve.yaml
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

name: Auto Approve
on:
  pull_request:
jobs:
  approve:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
    steps:
      - name: Check if PR author is admin
        id: check-admin
        run: |
          PR_AUTHOR="${{ github.event.pull_request.user.login }}"
          REPO="${{ github.repository }}"
          PERMISSION=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            "https://api.github.com/repos/$REPO/collaborators/$PR_AUTHOR/permission" \
            | jq -r '.permission')
          
          if [ "$PERMISSION" == "admin" ]; then
            echo "is_admin=true" >> $GITHUB_OUTPUT
            echo "PR author $PR_AUTHOR is an admin"
          else
            echo "is_admin=false" >> $GITHUB_OUTPUT
            echo "PR author $PR_AUTHOR is not an admin (permission: $PERMISSION)"
          fi
      - name: Approve Pull Request
        if: steps.check-admin.outputs.is_admin == 'true'
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.pulls.createReview({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.issue.number,
              event: 'APPROVE'
            })

================
File: .github/CODEOWNERS
================
* @KoheiYamashita

================
File: .github/release.yml
================
changelog:
  categories:
    - title: 🚀 Features
      labels:
        - '*'

================
File: app/api/v1/__init__.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

================
File: app/api/v1/agents.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from fastapi import APIRouter, Depends, HTTPException
from app.auth.api_key import get_api_key
from app.core.agent_manager import get_agent_manager
from app.schemas.agent import AgentResponse
import logging

router = APIRouter()
logger = logging.getLogger(__name__)


@router.get("/agents")
async def list_agents(api_key: str = Depends(get_api_key)):
    try:
        agent_manager = get_agent_manager()
        agents = agent_manager.get_all_agents()
        return [AgentResponse(agent_id=id, agent_name=name) for id, name in agents]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching agents: {str(e)}")

================
File: app/api/v1/chat.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from fastapi import APIRouter, Depends, Form, HTTPException, UploadFile, File, Request
from app.core.schedule_service import ScheduleService
from app.core.service_factory import ServiceFactory
from app.auth.api_key import get_api_key
from app.core.llm_service import LLMService
from app.core.tts_service import TTSService
from app.core.stt_service import STTService
from app.core.agent_manager import get_agent_manager
from app.schemas.agent import AgentConfig
from app.schemas.llm import LLMResponse
from app.schemas.schedule import ScheduleItem
from app.schemas.status import AgentStatus, CommunicationChannel
from app.utils.audio import calculate_audio_duration, upload_to_storage
import logging
from starlette.responses import FileResponse
from pathlib import Path
from typing import Optional
from app.core.config import get_settings
from app.schemas.chat import TextChatResponse, VoiceChatResponse

router = APIRouter()
logger = logging.getLogger(__name__)
service_factory = ServiceFactory()
settings = get_settings()
UPLOAD_DIR = settings.chat_audio_files_dir
MAX_FILES = settings.chat_max_audio_files


@router.post("/text/text")
async def chat_text_to_text(
    agent_id: str = Form(...),
    message: str = Form(...),
    channel: str = Form(...),
    force_generate: bool = Form(...),
    image_file: Optional[UploadFile] = None,
    api_key: str = Depends(get_api_key),
    llm_service: LLMService = Depends(service_factory.get_llm_service),
    schedule_service: ScheduleService = Depends(service_factory.get_schedule_service),
):
    agent_manager = get_agent_manager()
    agent_config = agent_manager.get_agent(agent_id)

    if not agent_config:
        raise HTTPException(status_code=404, detail=f"Agent ID {agent_id} not found")

    if image_file is not None:
        image_content = await image_file.read()
    else:
        image_content = None

    try:
        llm_response = await _create_llm_response(
            schedule_service,
            llm_service,
            agent_config,
            CommunicationChannel(channel),
            message,
            force_generate,
            image_content,
        )

        return TextChatResponse(
            user_message=llm_response.user_message,
            agent_message=llm_response.agent_message,
            emotion=llm_response.emotion,
        )
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error processing request: {str(e)}"
        )


@router.post("/text/voice")
async def chat_text_to_voice(
    request: Request,
    agent_id: str = Form(...),
    message: str = Form(...),
    channel: str = Form(...),
    force_generate: bool = Form(...),
    image_file: Optional[UploadFile] = None,
    api_key: str = Depends(get_api_key),
    llm_service: LLMService = Depends(service_factory.get_llm_service),
    tts_service: TTSService = Depends(service_factory.get_tts_service),
    schedule_service: ScheduleService = Depends(service_factory.get_schedule_service),
):
    agent_manager = get_agent_manager()
    agent_config = agent_manager.get_agent(agent_id)

    if not agent_config:
        raise HTTPException(status_code=404, detail=f"Agent ID {agent_id} not found")

    if image_file is not None:
        image_content = await image_file.read()
    else:
        image_content = None

    try:
        llm_response = await _create_llm_response(
            schedule_service,
            llm_service,
            agent_config,
            CommunicationChannel(channel),
            message,
            force_generate,
            image_content,
        )

        audio_data = await tts_service.generate_speech(
            llm_response.agent_message, agent_config
        )

        scheme = request.headers.get("X-Forwarded-Proto", "http")
        server_host = request.headers.get("X-Forwarded-Host", request.base_url.hostname)
        base_url = f"{scheme}://{server_host}"

        audio_url = await upload_to_storage(
            base_url, audio_data, "chat", UPLOAD_DIR, MAX_FILES
        )

        duration = calculate_audio_duration(audio_data)
        return VoiceChatResponse(
            user_message=message,
            agent_message=llm_response.agent_message,
            emotion=llm_response.emotion,
            audio_url=audio_url,
            duration=duration,
        )
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error processing request: {str(e)}"
        )


@router.post("/voice/text")
async def chat_voice_to_text(
    agent_id: str = Form(...),
    channel: str = Form(...),
    force_generate: bool = Form(...),
    image_file: Optional[UploadFile] = None,
    audio_file: UploadFile = File(...),
    api_key: str = Depends(get_api_key),
    llm_service: LLMService = Depends(service_factory.get_llm_service),
    stt_service: STTService = Depends(service_factory.get_stt_service),
    schedule_service: ScheduleService = Depends(service_factory.get_schedule_service),
):
    agent_manager = get_agent_manager()
    agent_config = agent_manager.get_agent(agent_id)

    if not agent_config:
        raise HTTPException(status_code=404, detail=f"Agent ID {agent_id} not found")

    if image_file is not None:
        image_content = await image_file.read()
    else:
        image_content = None

    try:
        audio_content = await audio_file.read()

        text_message = await stt_service.transcribe_audio(audio_content, agent_config)

        llm_response = await _create_llm_response(
            schedule_service,
            llm_service,
            agent_config,
            CommunicationChannel(channel),
            text_message,
            force_generate,
            image_content,
        )

        return TextChatResponse(
            user_message=llm_response.user_message,
            agent_message=llm_response.agent_message,
            emotion=llm_response.emotion,
        )
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error processing request: {str(e)}"
        )


@router.post("/voice/voice")
async def chat_voice_to_voice(
    request: Request,
    agent_id: str = Form(...),
    channel: str = Form(...),
    force_generate: bool = Form(...),
    image_file: Optional[UploadFile] = None,
    audio_file: UploadFile = File(...),
    api_key: str = Depends(get_api_key),
    llm_service: LLMService = Depends(service_factory.get_llm_service),
    stt_service: STTService = Depends(service_factory.get_stt_service),
    tts_service: TTSService = Depends(service_factory.get_tts_service),
    schedule_service: ScheduleService = Depends(service_factory.get_schedule_service),
):
    agent_manager = get_agent_manager()
    agent_config = agent_manager.get_agent(agent_id)

    if not agent_config:
        raise HTTPException(status_code=404, detail=f"Agent ID {agent_id} not found")

    if image_file is not None:
        image_content = await image_file.read()
    else:
        image_content = None

    try:
        audio_content = await audio_file.read()

        text_message = await stt_service.transcribe_audio(audio_content, agent_config)

        llm_response = await _create_llm_response(
            schedule_service,
            llm_service,
            agent_config,
            CommunicationChannel(channel),
            text_message,
            force_generate,
            image_content,
        )

        audio_data = await tts_service.generate_speech(
            llm_response.agent_message, agent_config
        )

        scheme = request.headers.get("X-Forwarded-Proto", "http")
        server_host = request.headers.get("X-Forwarded-Host", request.base_url.hostname)
        base_url = f"{scheme}://{server_host}"

        audio_url = await upload_to_storage(
            base_url, audio_data, "chat", UPLOAD_DIR, MAX_FILES
        )
        duration = calculate_audio_duration(audio_data)

        return VoiceChatResponse(
            user_message=text_message,
            agent_message=llm_response.agent_message,
            emotion=llm_response.emotion,
            audio_url=audio_url,
            duration=duration,
        )
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error processing request: {str(e)}"
        )


@router.get(f"/{UPLOAD_DIR}/{{file_name}}")
async def get_audio(file_name: str):
    if ".." in file_name or "/" in file_name:
        raise HTTPException(status_code=400, detail="Invalid file name")
    file_path = Path(f"{UPLOAD_DIR}/{file_name}.wav")
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Audio file not found")
    return FileResponse(file_path)


async def _create_llm_response(
    schedule_service: ScheduleService,
    llm_service: LLMService,
    agent_config: AgentConfig,
    channel: CommunicationChannel,
    message: str,
    force_generate: bool,
    image_content: Optional[bytes] = None,
) -> LLMResponse:
    isAvailable = schedule_service.get_current_availability(
        agent_config=agent_config,
        channel=channel,
    )
    current_schedule = schedule_service.get_current_schedule(agent_config.id)
    if not isAvailable and not force_generate:
        return await llm_service.generate_status_response(
            message=message,
            schedule=current_schedule,
            agent_config=agent_config,
        )

    elif not isAvailable and force_generate:
        if current_schedule:
            schedule_item = ScheduleItem(
                start_time=current_schedule.start_time,
                end_time=current_schedule.end_time,
                activity="Talking",
                status=AgentStatus.AVAILABLE,
                description="Talk to users.",
                location="my home",
            )
            await schedule_service.update_current_schedule(
                agent_id=agent_config.id,
                schedule_item=schedule_item,
            )

    return await llm_service.generate_response(
        message=message,
        schedule=current_schedule,
        agent_config=agent_config,
        image=image_content,
    )

================
File: app/api/v1/line.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
import aiohttp
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, Request
from starlette.requests import ClientDisconnect
from starlette.responses import FileResponse
from app.core.llm_service import LLMService
from app.core.schedule_service import ScheduleService
from app.core.service_factory import ServiceFactory
from app.core.tts_service import TTSService
from app.core.stt_service import STTService
from app.schemas.status import CommunicationChannel
from app.utils.audio import calculate_audio_duration, upload_to_storage
from pathlib import Path
from typing import Dict, cast, List
from app.schemas.agent import AgentConfig
from app.core.agent_manager import get_agent_manager
from app.core.config import get_settings
import logging
from linebot import AsyncLineBotApi  # type: ignore
from linebot.aiohttp_async_http_client import AiohttpAsyncHttpClient  # type: ignore
from linebot.v3.messaging.async_api_client import AsyncApiClient  # type: ignore
from linebot.v3.messaging import AsyncMessagingApi  # type: ignore
from linebot.v3.webhook import WebhookParser  # type: ignore
from linebot.v3.webhooks.models import Event  # type: ignore
from linebot.v3.messaging.models import (  # type: ignore
    ReplyMessageRequest,
    TextMessage,
    AudioMessage,
)
from linebot.v3.exceptions import (  # type: ignore
    InvalidSignatureError,
)
from linebot.v3.webhooks import (  # type: ignore
    MessageEvent,
    TextMessageContent,
    ImageMessageContent,
    Configuration,
)

router = APIRouter()
logger = logging.getLogger(__name__)
service_factory = ServiceFactory()
settings = get_settings()
UPLOAD_DIR = settings.line_audio_files_dir
MAX_FILES = settings.line_max_audio_files
user_image_cache: Dict[str, bytes] = {}


async def process_line_events_background(
    body: str,
    signature: str,
    agent_config: AgentConfig,
    request: Request,
    llm_service: LLMService,
    tts_service: TTSService,
    schedule_service: ScheduleService,
):
    aio_session = aiohttp.ClientSession()
    async_client: AsyncApiClient | None = None
    try:
        configuration = Configuration(
            access_token=agent_config.line_channel_access_token
        )
        async_client = AsyncApiClient(configuration)
        line_messaging_api = AsyncMessagingApi(async_client)

        aio_client = AiohttpAsyncHttpClient(aio_session)
        line_bot_api = AsyncLineBotApi(
            channel_access_token=agent_config.line_channel_access_token,
            async_http_client=aio_client,
        )

        events = parse_line_events(body, signature, agent_config.line_channel_secret)
        try:
            for event in events:
                if not isinstance(event, MessageEvent):
                    continue

                if isinstance(event.message, ImageMessageContent):
                    await process_image_message(line_bot_api, event)
                    continue
                elif isinstance(event.message, TextMessageContent):
                    text_message = event.message.text
                else:
                    continue
                cached_image_bytes = user_image_cache.pop(event.source.user_id, None)  # type: ignore
                isAvailable = schedule_service.get_current_availability(
                    agent_config=agent_config, channel=CommunicationChannel.CHAT
                )
                current_schedule = schedule_service.get_current_schedule(
                    agent_config.id
                )
                if not isAvailable:
                    llm_response = await llm_service.generate_status_response(
                        message=text_message,
                        schedule=current_schedule,
                        agent_config=agent_config,
                    )
                else:
                    llm_response = await llm_service.generate_response(
                        text_message,
                        current_schedule,
                        agent_config,
                        image=cached_image_bytes,
                    )
                audio_data = await tts_service.generate_speech(
                    llm_response.agent_message, agent_config
                )
                scheme = request.headers.get("X-Forwarded-Proto", "http")
                server_host = request.headers.get(
                    "X-Forwarded-Host", request.base_url.hostname
                )
                base_url = f"{scheme}://{server_host}"
                audio_url = await upload_to_storage(
                    base_url, audio_data, "line", UPLOAD_DIR, MAX_FILES
                )
                duration = calculate_audio_duration(audio_data)

                await line_messaging_api.reply_message(  # type: ignore
                    ReplyMessageRequest(
                        reply_token=event.reply_token,  # type: ignore
                        messages=[
                            TextMessage(text=llm_response.agent_message),  # type: ignore
                            AudioMessage(
                                original_content_url=audio_url,  # type: ignore
                                duration=duration,
                            ),
                        ],
                    )
                )
        except Exception as e:
            logger.exception(
                f"Error in process_line_events_background: {str(e)}", exc_info=True
            )
    finally:
        await aio_session.close()
        if async_client is not None:
            await async_client.close()


@router.post("/callback/{agent_id}")
async def handle_line_callback(
    background_tasks: BackgroundTasks,
    request: Request,
    agent_id: str,
    llm_service: LLMService = Depends(service_factory.get_llm_service),
    tts_service: TTSService = Depends(service_factory.get_tts_service),
    stt_service: STTService = Depends(service_factory.get_stt_service),
    schedule_service: ScheduleService = Depends(service_factory.get_schedule_service),
):
    signature, body = await extract_line_request_data(request)
    agent_manager = get_agent_manager()
    try:
        agent_config = agent_manager.get_agent(agent_id)
    except KeyError:
        raise HTTPException(
            status_code=404, detail=f"Agent with ID '{agent_id}' not found."
        )

    # Verify signature before returning OK
    parse_line_events(body, signature, agent_config.line_channel_secret)

    background_tasks.add_task(
        process_line_events_background,
        body,
        signature,
        agent_config,
        request,
        llm_service,
        tts_service,
        schedule_service,
    )
    return "OK"


async def extract_line_request_data(request: Request):
    try:
        signature = request.headers.get("X-Line-Signature")
        if not signature:
            raise HTTPException(
                status_code=400, detail="X-Line-Signature header is missing"
            )

        body = await request.body()
        return signature, body.decode()
    except ClientDisconnect:
        logger.warning("Client disconnected while reading request body")
        raise HTTPException(status_code=400, detail="Client disconnected")
    except Exception as e:
        logger.error(f"Error extracting LINE request data: {str(e)}")
        raise HTTPException(status_code=400, detail="Invalid request")


def parse_line_events(
    body: str, signature: str, line_channel_secret: str
) -> List[Event]:
    line_parser = WebhookParser(line_channel_secret)

    try:
        events: List[Event] = cast(List[Event], line_parser.parse(body, signature))  # type: ignore
    except InvalidSignatureError:
        raise HTTPException(status_code=400, detail="Invalid signature")
    return events


async def process_image_message(line_bot_api: AsyncLineBotApi, event: MessageEvent):
    message_content = await line_bot_api.get_message_content(event.message.id)  # type: ignore
    image_bytes: bytes = b""
    async for chunk in message_content.iter_content():  # type: ignore
        assert isinstance(chunk, bytes)
        image_bytes += chunk
    user_image_cache[event.source.user_id] = image_bytes  # type: ignore


@router.get(f"/{UPLOAD_DIR}/{{file_name}}")
async def get_audio(file_name: str):
    if ".." in file_name or "/" in file_name:
        raise HTTPException(status_code=400, detail="Invalid file name")
    file_path = Path(f"{UPLOAD_DIR}/{file_name}.wav")
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Audio file not found")
    return FileResponse(file_path)

================
File: app/api/v1/schedule.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException
from typing import Dict, Optional, List
from app.auth.api_key import get_api_key
from app.core.agent_manager import get_agent_manager
from app.core.schedule_service import ScheduleService
from app.core.service_factory import ServiceFactory
from app.schemas.status import AgentStatus
from app.schemas.schedule import ScheduleItem

router = APIRouter()
service_factory = ServiceFactory()


@router.put("/{agent_id}/update")
async def update_agent_schedule(
    agent_id: str,
    activity: str,
    status: AgentStatus,
    end_time: Optional[str] = None,
    description: Optional[str] = None,
    location: Optional[str] = None,
    api_key: str = Depends(get_api_key),
    schedule_service: ScheduleService = Depends(service_factory.get_schedule_service),
) -> ScheduleItem:
    """Update the status of an agent"""
    agent_manager = get_agent_manager()

    try:
        agent_config = agent_manager.get_agent(agent_id=agent_id)
        current_schedule = schedule_service.get_current_schedule(agent_config.id)
        if current_schedule:
            if end_time:
                current_end_time: datetime = datetime.strptime(
                    end_time, "%Y-%m-%dT%H:%M:%S"
                )
            else:
                current_end_time: datetime = current_schedule.end_time
            schedule_item = ScheduleItem(
                start_time=current_schedule.start_time,
                end_time=current_end_time,
                activity=activity,
                status=status,
                description=description,
                location=location,
            )
            await schedule_service.update_current_schedule(
                agent_id=agent_config.id,
                schedule_item=schedule_item,
            )
            return schedule_item
        else:
            raise HTTPException(status_code=404, detail="Schedule not found")
    except KeyError:
        raise HTTPException(status_code=404, detail="Agent not found")


@router.get("/{agent_id}")
async def get_agent_schedule(
    agent_id: str,
    api_key: str = Depends(get_api_key),
    schedule_service: ScheduleService = Depends(service_factory.get_schedule_service),
) -> ScheduleItem:
    """Get the current day's schedule for an agent"""
    agent_manager = get_agent_manager()
    try:
        agent_manager.get_agent(agent_id)  # Verify agent exists
        schedule = schedule_service.get_current_schedule(agent_id)
        if not schedule:
            raise HTTPException(
                status_code=404,
                detail="Schedule not found. Wait for next schedule generation.",
            )
        return schedule
    except KeyError:
        raise HTTPException(status_code=404, detail="Agent not found")


@router.get("/history/{agent_id}")
async def get_agent_schedule_history(
    agent_id: str,
    api_key: str = Depends(get_api_key),
    schedule_service: ScheduleService = Depends(service_factory.get_schedule_service),
) -> Optional[List[ScheduleItem]]:
    """Get the schedule history for an agent"""
    agent_manager = get_agent_manager()
    try:
        agent_manager.get_agent(agent_id)
        history = await schedule_service.get_agent_schedule_history(agent_id)
        return history
    except KeyError:
        raise HTTPException(status_code=404, detail="Agent not found")


@router.get("/agts/{agent_id}/availability")
async def get_agent_availability(
    agent_id: str,
    api_key: str = Depends(get_api_key),
    schedule_service: ScheduleService = Depends(service_factory.get_schedule_service),
) -> Dict[str, bool]:
    """Get the current availability of all communication channels for an agent"""
    agent_manager = get_agent_manager()
    try:
        agent = agent_manager.get_agent(agent_id)
        from app.schemas.status import CommunicationChannel

        availability = {
            "chat": schedule_service.get_current_availability(
                agent, CommunicationChannel.CHAT
            ),
            "voice": schedule_service.get_current_availability(
                agent, CommunicationChannel.VOICE
            ),
            "video": schedule_service.get_current_availability(
                agent, CommunicationChannel.VIDEO
            ),
        }
        return availability
    except KeyError:
        raise HTTPException(status_code=404, detail="Agent not found")

================
File: app/api/v1/web_socket.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
import base64
import io
import json
import logging
from pathlib import Path
from typing import Optional
import secrets
import time
from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import FileResponse, HTMLResponse
from fastapi.websockets import WebSocket
from jsonschema import ValidationError
from app.core.agent_manager import get_agent_manager
from app.core.config import get_settings
from app.core.schedule_service import ScheduleService
from app.core.service_factory import ServiceFactory
from app.schemas.agent import AgentConfig
from app.schemas.llm import LLMResponse
from app.schemas.schedule import ScheduleItem
from app.schemas.status import AgentStatus, CommunicationChannel
from app.schemas.web_socket import (
    AudioRequest,
    AudioResponse,
    ImageAudioRequest,
    ImageTextRequest,
    TextRequest,
    TextResponse,
    TokenResponse,
)
from app.utils.audio import calculate_audio_duration, upload_to_storage
from app.core.llm_service import LLMService
from app.core.stt_service import STTService
from app.core.tts_service import TTSService
from app.auth.api_key import get_api_key

router = APIRouter()
settings = get_settings()
service_factory = ServiceFactory()
logger = logging.getLogger(__name__)
UPLOAD_DIR = settings.web_socket_audio_files_dir
MAX_FILES = settings.web_socket_max_audio_files

ws_tokens: dict[str, tuple[str, float]] = {}
TOKEN_LIFETIME = 10


# TODO: トークン取得時にAgentIdとComunitcationChannelを指定する
@router.get("/get_ws_token")
async def get_ws_token(api_key: str = Depends(get_api_key)):
    clean_expired_tokens()
    token = secrets.token_urlsafe(16)
    expire_at = time.time() + TOKEN_LIFETIME
    ws_tokens[token] = (api_key, expire_at)
    return TokenResponse(token=token, expire_in=TOKEN_LIFETIME)


@router.websocket("")
async def websocket_endpoint(
    websocket: WebSocket,
    token: Optional[str] = Query(None),
    llm_service: LLMService = Depends(service_factory.get_llm_service),
    stt_service: STTService = Depends(service_factory.get_stt_service),
    tts_service: TTSService = Depends(service_factory.get_tts_service),
    schedule_service: ScheduleService = Depends(service_factory.get_schedule_service),
):
    clean_expired_tokens()
    if not token or token not in ws_tokens:
        await websocket.close(code=1008)
        return
    api_key, expire_at = ws_tokens[token]
    if time.time() > expire_at:
        del ws_tokens[token]
        await websocket.close(code=1008)
        return
    del ws_tokens[token]
    await websocket.accept()
    await websocket.send_text(
        f"Hello! Connected with token associated to API key: {api_key}"
    )
    agent_manager = get_agent_manager()
    while True:
        try:
            message = await websocket.receive_text()
            raw_data = json.loads(message)
            msg_type = raw_data.get("request_type")
            request_obj = None
            if msg_type == "text":
                request_obj = TextRequest(**raw_data)
            elif msg_type == "audio":
                request_obj = AudioRequest(**raw_data)
            elif msg_type == "image_text":
                request_obj = ImageTextRequest(**raw_data)
            elif msg_type == "image_audio":
                request_obj = ImageAudioRequest(**raw_data)
            else:
                await websocket.send_text(
                    json.dumps({"type": "text", "text": "unknown request type"})
                )
                continue

            agent_config = agent_manager.get_agent(request_obj.agent_id)
            image_content: Optional[bytes] = None
            text_message: str = ""

            if isinstance(request_obj, AudioRequest):
                audio_bytes = base64.b64decode(request_obj.audio)
                audio_file = io.BytesIO(audio_bytes)
                audio_content = audio_file.read()
                text_message = await stt_service.transcribe_audio(
                    audio_content, agent_config
                )
            elif isinstance(request_obj, ImageAudioRequest):
                audio_bytes = base64.b64decode(request_obj.audio)
                audio_file = io.BytesIO(audio_bytes)
                audio_content = audio_file.read()
                text_message = await stt_service.transcribe_audio(
                    audio_content, agent_config
                )
                image_bytes = base64.b64decode(request_obj.image)
                image_file = io.BytesIO(image_bytes)
                image_content = image_file.read()
            elif isinstance(request_obj, ImageTextRequest):
                text_message = request_obj.text
                image_bytes = base64.b64decode(request_obj.image)
                image_file = io.BytesIO(image_bytes)
                image_content = image_file.read()
            else:
                text_message = request_obj.text
            llm_response = await _create_llm_response(
                schedule_service,
                llm_service,
                agent_config,
                CommunicationChannel.VOICE,
                message,
                request_obj.force_generate,
                image_content,
            )

            agent_message = llm_response.agent_message.rstrip("\n")
            emotion = llm_response.emotion

            if request_obj.responce_type == "text":
                response = TextResponse(
                    user_message=text_message,
                    agent_message=agent_message,
                    emotion=emotion,
                )
            elif request_obj.responce_type == "audio":
                audio_data = await tts_service.generate_speech(
                    agent_message, agent_config
                )

                scheme = websocket.headers.get("X-Forwarded-Proto", "http")
                server_host = websocket.headers.get(
                    "X-Forwarded-Host", websocket.base_url.hostname
                )
                base_url = f"{scheme}://{server_host}"

                audio_url = await upload_to_storage(
                    base_url, audio_data, "ws", UPLOAD_DIR, MAX_FILES
                )
                duration = calculate_audio_duration(audio_data)
                response = AudioResponse(
                    user_message=text_message,
                    agent_message=agent_message,
                    emotion=emotion,
                    audio_url=audio_url,
                    duration=duration,
                )
            else:
                await websocket.send_text(
                    json.dumps({"type": "text", "text": "unknown response type"})
                )
                continue

            await websocket.send_text(response.model_dump_json())
        except ValidationError as e:
            error_msg = f"Invalid message format: {e}"
            await websocket.send_text(json.dumps({"type": "text", "text": error_msg}))
        except Exception as e:
            error_msg = f"Exception format: {e}"
            await websocket.send_text(json.dumps({"type": "text", "text": error_msg}))
            await websocket.close()
            break


@router.get(f"/{UPLOAD_DIR}/{{file_name}}")
async def get_audio(file_name: str):
    if ".." in file_name or "/" in file_name:
        raise HTTPException(status_code=400, detail="Invalid file name")
    file_path = Path(f"{UPLOAD_DIR}/{file_name}.wav")
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Audio file not found")
    return FileResponse(file_path)


def clean_expired_tokens():
    now = time.time()
    expired = [t for t, (_, exp) in ws_tokens.items() if exp < now]
    for t in expired:
        del ws_tokens[t]


@router.get("/ws-test", response_class=HTMLResponse)
def ws_test_page():
    initial_message = json.dumps(
        {
            "request_type": "text",
            "responce_type": "text",
            "agent_id": "1",
            "force_generate": "true",
            "text": "Hello",
        },
        ensure_ascii=False,
        indent=2,
    )
    return f"""
    <!DOCTYPE html>
    <html>
    <head><title>WebSocket Test</title></head>
    <body>
        <h1>WebSocket Test Page</h1>
        <div>
            <label for="tokenInput">Token: </label>
            <input type="text" id="tokenInput" size="40" placeholder="Enter your WebSocket token here" />
            <button id="connectBtn">Connect</button>
        </div>
        <hr/>

        <div>
            <p>Enter your JSON message in the text area below and click the "Send" button to submit it.</p>
            <p>If you want to attach an image, select a file. Once selected, the request_type will be automatically changed to "image_text".</p>
            <textarea id="msgInput" rows="10" cols="60">{initial_message}</textarea><br>
            <input type="file" id="fileInput" accept="image/*"/><br><br>
            <button id="sendBtn" disabled>Send</button>
        </div>

        <pre id="log"></pre>

        <script>
            let ws = null;

            const logArea = document.getElementById('log');
            const connectBtn = document.getElementById('connectBtn');
            const sendBtn = document.getElementById('sendBtn');
            const fileInput = document.getElementById('fileInput');
            const msgInput = document.getElementById('msgInput');
            const tokenInput = document.getElementById('tokenInput');

            let imageBase64 = null;

            connectBtn.onclick = () => {{
                if (ws && ws.readyState === WebSocket.OPEN) {{
                    ws.close();
                }}

                const token = tokenInput.value.trim();
                if (!token) {{
                    logArea.textContent += "[Error] Token is empty.\\n";
                    return;
                }}

                const protocol = (window.location.protocol === 'https:') ? 'wss:' : 'ws:';
                const wsUrl = protocol + '//' + window.location.host + '/v1/ws?token=' + encodeURIComponent(token);

                ws = new WebSocket(wsUrl);

                ws.onopen = () => {{
                    logArea.textContent += "WebSocket connection opened with token: " + token + "\\n";
                    sendBtn.disabled = false;
                }};

                ws.onmessage = (event) => {{
                    logArea.textContent += "Received: " + event.data + "\\n";
                }};

                ws.onclose = () => {{
                    logArea.textContent += "WebSocket connection closed\\n";
                    sendBtn.disabled = true;
                }};

                ws.onerror = (err) => {{
                    logArea.textContent += "[Error] " + err + "\\n";
                }};
            }};

            fileInput.onchange = (event) => {{
                const file = event.target.files[0];
                if (!file) {{
                    imageBase64 = null;
                    return;
                }}

                const reader = new FileReader();
                reader.onload = () => {{
                    imageBase64 = reader.result.split(",")[1];

                    let msgObj;
                    try {{
                        msgObj = JSON.parse(msgInput.value);
                    }} catch (e) {{
                        logArea.textContent += "JSON Parse Error: " + e + "\\n";
                        return;
                    }}

                    msgObj.request_type = "image_text";
                    msgObj.image = imageBase64;

                    msgInput.value = JSON.stringify(msgObj, null, 2);
                }};
                reader.readAsDataURL(file);
            }};

            sendBtn.onclick = () => {{
                if (!ws || ws.readyState !== WebSocket.OPEN) {{
                    logArea.textContent += "[Error] WebSocket is not connected.\\n";
                    return;
                }}
                const msg = msgInput.value;
                ws.send(msg);
                logArea.textContent += "Sent: " + msg + "\\n";
            }};
        </script>
    </body>
    </html>
    """


async def _create_llm_response(
    schedule_service: ScheduleService,
    llm_service: LLMService,
    agent_config: AgentConfig,
    channel: CommunicationChannel,
    message: str,
    force_generate: bool,
    image_content: Optional[bytes] = None,
) -> LLMResponse:
    isAvailable = schedule_service.get_current_availability(
        agent_config=agent_config, channel=CommunicationChannel.CHAT
    )
    current_schedule = schedule_service.get_current_schedule(agent_config.id)
    if not isAvailable and not force_generate:
        return await llm_service.generate_status_response(
            message=message,
            schedule=current_schedule,
            agent_config=agent_config,
        )

    elif not isAvailable and force_generate:
        if current_schedule:
            schedule_item = ScheduleItem(
                start_time=current_schedule.start_time,
                end_time=current_schedule.end_time,
                activity="Talking",
                status=AgentStatus.AVAILABLE,
                description="Talk to users.",
                location="my home",
            )
            await schedule_service.update_current_schedule(
                agent_id=agent_config.id,
                schedule_item=schedule_item,
            )

    return await llm_service.generate_response(
        message=message,
        schedule=current_schedule,
        agent_config=agent_config,
        image=image_content,
    )

================
File: app/api/__init__.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

================
File: app/auth/__init__.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

================
File: app/auth/api_key.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from fastapi import Security, HTTPException, Depends
from fastapi.security.api_key import APIKeyHeader
from starlette.status import HTTP_403_FORBIDDEN
from typing import Optional
from app.core.config import get_settings, Settings

API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)


async def get_api_key(
    api_key_header: Optional[str] = Security(api_key_header),
    settings: Settings = Depends(get_settings),
) -> str:
    if api_key_header is None:
        raise HTTPException(
            status_code=HTTP_403_FORBIDDEN, detail="API key is required"
        )

    if not settings.api_keys:
        raise HTTPException(
            status_code=HTTP_403_FORBIDDEN, detail="No API keys configured"
        )

    if not settings.is_valid_api_key(api_key_header):
        raise HTTPException(status_code=HTTP_403_FORBIDDEN, detail="Invalid API key")

    return api_key_header

================
File: app/core/__init__.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

================
File: app/core/agent_manager.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from functools import lru_cache
from typing import Dict, List, Tuple
from app.schemas.agent import AgentConfig
from app.core.config import get_settings
from app.schemas.schedule import AgentScheduleConfig


class AgentManager:
    def __init__(self):
        self.settings = get_settings()
        self.agents: Dict[str, AgentConfig] = self._load_agents_from_env()

    def _load_agents_from_env(self) -> Dict[str, AgentConfig]:
        agents: Dict[str, AgentConfig] = {}
        i = 1
        while True:
            name = self.settings.get_agent_env(i, "NAME")
            message_generate_llm_api_key = self.settings.get_agent_env(
                i, "MESSAGE_GENERATE_LLM_API_KEY"
            )
            message_generate_llm_model = self.settings.get_agent_env(
                i, "MESSAGE_GENERATE_LLM_MODEL"
            )
            analyze_generate_llm_api_key = self.settings.get_agent_env(
                i, "ANALYZE_GENERATE_LLM_API_KEY"
            )
            analyze_generate_llm_model = self.settings.get_agent_env(
                i, "ANALYZE_GENERATE_LLM_MODEL"
            )
            schedule_generate_llm_api_key = self.settings.get_agent_env(
                i, "SCHEDULE_GENERATE_LLM_API_KEY"
            )
            schedule_generate_llm_model = self.settings.get_agent_env(
                i, "SCHEDULE_GENERATE_LLM_MODEL"
            )
            required_values = [
                name,
                message_generate_llm_api_key,
                message_generate_llm_model,
                analyze_generate_llm_api_key,
                analyze_generate_llm_model,
                schedule_generate_llm_api_key,
                schedule_generate_llm_model,
            ]

            if not all(required_values):
                break

            agents[str(i)] = AgentConfig(
                id=str(i),
                name=name,
                message_generate_llm_api_key=message_generate_llm_api_key,
                message_generate_llm_model=message_generate_llm_model,
                analyze_generate_llm_api_key=analyze_generate_llm_api_key,
                analyze_generate_llm_model=analyze_generate_llm_model,
                schedule_generate_llm_api_key=schedule_generate_llm_api_key,
                schedule_generate_llm_model=schedule_generate_llm_model,
                message_generate_llm_base_url=self.settings.get_agent_env(
                    i, "MESSAGE_GENERATE_LLM_BASE_URL"
                )
                or "",
                analyze_generate_llm_base_url=self.settings.get_agent_env(
                    i, "ANALYZE_GENERATE_LLM_BASE_URL"
                )
                or "",
                schedule_generate_llm_base_url=self.settings.get_agent_env(
                    i, "SCHEDULE_GENERATE_LLM_BASE_URL"
                )
                or "",
                vision_generate_llm_base_url=self.settings.get_agent_env(
                    i, "VISION_GENERATE_LLM_BASE_URL"
                )
                or "",
                vision_generate_llm_api_key=self.settings.get_agent_env(
                    i, "VISION_GENERATE_LLM_API_KEY"
                )
                or "",
                vision_generate_llm_model=self.settings.get_agent_env(
                    i, "VISION_GENERATE_LLM_MODEL"
                )
                or "",
                llm_system_prompt=self.settings.get_agent_env(i, "LLM_SYSTEM_PROMPT")
                or "",
                tts_base_url=self.settings.get_agent_env(i, "TTS_BASE_URL") or "",
                tts_api_key=self.settings.get_agent_env(i, "TTS_API_KEY") or "",
                tts_type=self.settings.get_agent_env(i, "TTS_TYPE") or "",
                tts_speaker_model=self.settings.get_agent_env(i, "TTS_SPEAKER_MODEL")
                or "",
                tts_speaker_id=self.settings.get_agent_env(i, "TTS_SPEAKER_ID") or "",
                line_channel_secret=self.settings.get_agent_env(
                    i, "LINE_CHANNEL_SECRET"
                )
                or "",
                line_channel_access_token=self.settings.get_agent_env(
                    i, "LINE_CHANNEL_ACCESS_TOKEN"
                )
                or "",
                schedule=AgentScheduleConfig(),
            )
            i += 1
        return agents

    def get_agent(self, agent_id: str) -> AgentConfig:
        agent = self.agents.get(agent_id)
        if agent is None:
            raise KeyError(f"Agent with ID '{agent_id}' not found.")
        return agent

    def get_all_agents(self) -> List[Tuple[str, str]]:
        return [(id, config.name) for id, config in self.agents.items()]


@lru_cache()
def get_agent_manager() -> AgentManager:
    return AgentManager()

================
File: app/core/config.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from dotenv import load_dotenv
from functools import lru_cache
import os
from typing import List

from numpy import double


class Settings:
    def __init__(self):
        load_dotenv()
        self.environment = os.getenv("ENVIRONMENT", "development")
        self.api_keys: List[str] = [
            key.strip() for key in os.getenv("API_KEYS", "").split(",") if key.strip()
        ]
        self.line_max_audio_files = int(os.getenv("LINE_MAX_AUDIO_FILES", "5"))
        self.line_audio_files_dir = str(
            os.getenv("LINE_AUDIO_FILES_DIR", "line_audio_files")
        )
        self.chat_max_audio_files = int(os.getenv("CHAT_MAX_AUDIO_FILES", "5"))
        self.chat_audio_files_dir = str(
            os.getenv("CHAT_AUDIO_FILES_DIR", "chat_audio_files")
        )
        self.web_socket_max_audio_files = int(
            os.getenv("WEB_SOCKET_MAX_AUDIO_FILES", "5")
        )
        self.web_socket_audio_files_dir = str(
            os.getenv("WEB_SOCKET_AUDIO_FILES_DIR", "chat_audio_files")
        )
        self.check_support_vision_model = (
            os.getenv("CHECK_SUPPORT_VISION_MODEL", "True").lower() == "true"
        )
        self.redis_url = str(os.getenv("REDIS_URL", "redis://karakuri-redis"))
        self.redis_password = str(os.getenv("REDIS_PASSWORD", "Redis_P@ssw0rd123"))
        self.threshold_tokens_percentage = double(
            os.getenv("THRESHOLD_TOKENS_PERCENTAGE", 0.8)
        )

    def get_agent_env(self, agent_id: int, key: str) -> str:
        return os.getenv(f"AGENT_{agent_id}_{key}") or ""

    def is_valid_api_key(self, api_key: str) -> bool:
        return api_key in self.api_keys


@lru_cache()
def get_settings() -> Settings:
    return Settings()

================
File: app/core/llm_service.py
================
import asyncio
from typing import List, Optional, Union
import base64
import logging
import json
from litellm import (
    AllMessageValues,
    ChatCompletionImageObject,
    ChatCompletionImageUrlObject,
    ChatCompletionTextObject,
    ChatCompletionUserMessage,
    ChatCompletionSystemMessage,
    ChatCompletionAssistantMessage,
    Choices,
    CustomStreamWrapper,
    acompletion,  # type: ignore
    ModelResponse,  # type: ignore
    utils,  # type: ignore
)
from app.core.memory_service import MemoryService
from app.schemas.agent import AgentConfig
from app.schemas.emotion import Emotion
from app.schemas.llm import LLMResponse
from app.core.config import get_settings
from app.core.memory_service import conversation_history_lock
from app.schemas.schedule import ScheduleItem

logger = logging.getLogger(__name__)
settings = get_settings()
CHECK_SUPPORT_VISION_MODEL = settings.check_support_vision_model


class LLMService:
    def __init__(
        self,
        memory_service: MemoryService,
    ):
        self.memory_service = memory_service

    def create_emotion_analysis_prompt(self, text: str) -> str:
        emotions = Emotion.to_request_values()
        return f"""analyze the following text emotion.
        Respond ONLY in the specified JSON format without any additional explanation.

        Input text: {text}

        Required JSON format:
        {{
            "emotion": One of these emotions: {', '.join(emotions)}
        }}"""

    def create_system_prompt(
        self, llm_system_prompt: str, schedule: Optional[ScheduleItem]
    ) -> str:
        return f"""
        {llm_system_prompt}

        Craft a natural, contextual response based on your current status.

        Current Schedule: {schedule}
        """

    async def generate_response(
        self,
        message: str,
        schedule: Optional[ScheduleItem],
        agent_config: AgentConfig,
        image: Optional[bytes] = None,
    ) -> LLMResponse:
        async with conversation_history_lock:
            conversation_history = await self.memory_service.get_conversation_history(
                agent_config.id
            )
            systemMessage = ChatCompletionSystemMessage(
                role="system",
                content=self.create_system_prompt(
                    agent_config.llm_system_prompt, schedule
                ),
            )

            if image:
                image_data_b64 = base64.b64encode(image).decode("utf-8")
                data_url = f"data:image/jpeg;base64,{image_data_b64}"

                if not CHECK_SUPPORT_VISION_MODEL or utils.supports_vision(
                    model=agent_config.message_generate_llm_model
                ):
                    conversation_history.append(
                        ChatCompletionUserMessage(
                            role="user",
                            content=[
                                ChatCompletionImageObject(
                                    type="image_url",
                                    image_url=ChatCompletionImageUrlObject(
                                        url=data_url
                                    ),
                                ),
                                ChatCompletionTextObject(type="text", text=message),
                            ],
                        )
                    )
                else:
                    image_description = await self.convert_images_to_text(
                        data_url, agent_config
                    )
                    conversation_history.append(
                        ChatCompletionUserMessage(
                            role="user",
                            content=f"{message}\n\n[Image description: {image_description}]",
                        )
                    )
            else:
                conversation_history.append(
                    ChatCompletionUserMessage(
                        role="user",
                        content=message,
                    )
                )

            response = await acompletion(
                base_url=agent_config.message_generate_llm_base_url,
                api_key=agent_config.message_generate_llm_api_key,
                model=agent_config.message_generate_llm_model,
                messages=[systemMessage] + conversation_history[:],
            )
            agent_message = self.get_message_content(response)
            conversation_history.append(
                ChatCompletionAssistantMessage(
                    role="assistant",
                    content=agent_message,
                )
            )

            emotion = await self.generate_emotion_response(
                user_message=message,
                agent_message=agent_message,
                agent_config=agent_config,
            )

            llm_response = LLMResponse(
                user_message=message, agent_message=agent_message, emotion=emotion
            )

            asyncio.create_task(
                self.memory_service.update_conversation_history(
                    agent_config.message_generate_llm_model,
                    agent_config.id,
                    systemMessage,
                    conversation_history,
                )
            )

        return llm_response

    def get_message_content(
        self, response: Union[ModelResponse, CustomStreamWrapper]
    ) -> str:
        if not isinstance(response, ModelResponse):
            raise TypeError("response is not a ModelResponse instance.")
        if not hasattr(response, "choices"):
            raise AttributeError("response has no 'choices' attribute.")
        if len(response.choices) == 0:
            raise IndexError("response.choices is empty, no elements to access.")

        first_choice = response.choices[0]
        if not isinstance(first_choice, Choices):
            raise TypeError("response.choices[0] is not an instance of Choices.")

        content = first_choice.message.content
        if not isinstance(content, str):
            raise TypeError("response.choices[0].message.content is not a string.")
        if not content.strip():
            raise ValueError(
                "response.choices[0].message.content is empty or whitespace-only."
            )

        return content

    async def convert_images_to_text(
        self, data_url: str, agent_config: AgentConfig
    ) -> str:
        vision_messages: List[AllMessageValues] = [
            ChatCompletionSystemMessage(
                role="system",
                content="You are a vision model that converts images into a descriptive text.",
            ),
            ChatCompletionUserMessage(
                role="user",
                content=[
                    ChatCompletionImageObject(
                        type="image_url",
                        image_url=ChatCompletionImageUrlObject(url=data_url),
                    ),
                    ChatCompletionTextObject(
                        type="text",
                        text="Please describe the following images in a concise and clear way.",
                    ),
                ],
            ),
        ]
        vision_response = await acompletion(
            base_url=agent_config.vision_generate_llm_base_url,
            api_key=agent_config.vision_generate_llm_api_key,
            model=agent_config.vision_generate_llm_model,
            messages=vision_messages,
        )
        return self.get_message_content(vision_response)

    async def generate_emotion_response(
        self,
        user_message: str,
        agent_message: str,
        agent_config: AgentConfig,
    ) -> str:
        emotion_prompt = self.create_emotion_analysis_prompt(agent_message)
        emotion_messages: List[AllMessageValues] = [
            ChatCompletionSystemMessage(
                role="system",
                content="You are an expert emotion analyzer. Always respond in the exact JSON format requested.",
            ),
            ChatCompletionUserMessage(
                role="user",
                content=emotion_prompt,
            ),
        ]
        emotion_response = await acompletion(
            base_url=agent_config.analyze_generate_llm_base_url,
            api_key=agent_config.analyze_generate_llm_api_key,
            model=agent_config.analyze_generate_llm_model,
            messages=emotion_messages,
            response_format={"type": "json_object"},
        )

        try:
            parsed_response = json.loads(self.get_message_content(emotion_response))
            if (
                not isinstance(parsed_response, dict)
                or "emotion" not in parsed_response
            ):
                raise ValueError("Invalid JSON structure")

            if parsed_response["emotion"] not in Emotion.to_request_values():
                raise ValueError(f"Invalid emotion value: {parsed_response['emotion']}")
            return parsed_response["emotion"]
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Error parsing emotion response: {str(e)}")
            return Emotion.NEUTRAL.value

    async def generate_next_schedule(
        self,
        prompt: str,
        agent_config: AgentConfig,
    ) -> str:
        """Generate the next schedule item using LLM"""
        system_prompt = """
        You are a schedule generator for an AI agent. Create the next schedule item considering:
        
        1. The current context and recent activities
        2. The agent's personality and daily routine
        3. Natural flow and transitions between activities
        4. Appropriate time allocation for the activity

        Required Output Format:
        {
            "start_time": "YYYY-MM-DD HH:MM",
            "end_time": "YYYY-MM-DD HH:MM",
            "activity": "Activity name",
            "status": "available/working/eating/etc",
            "description": "Brief description",
            "location": "Location"
        }

        Guidelines:
        - Times must be in 24-hour format (HH:MM)
        - Status must match one of the defined status types
        - Activity should be specific and meaningful
        - Description should provide context for the activity
        - Location should be specific when relevant
        """

        messages = [
            ChatCompletionSystemMessage(role="system", content=system_prompt),
            ChatCompletionUserMessage(role="user", content=prompt),
        ]

        response = await acompletion(
            base_url=agent_config.schedule_generate_llm_base_url,
            api_key=agent_config.schedule_generate_llm_api_key,
            model=agent_config.schedule_generate_llm_model,
            messages=messages,
            response_format={"type": "json_object"},
        )
        return self.get_message_content(response)

    async def generate_laungage(self, text: str, agent_config: AgentConfig) -> str:
        systemMessage = ChatCompletionSystemMessage(
            role="system",
            content="You are an expert language analyzer. Always respond in the exact JSON format requested.",
        )
        userMessage = ChatCompletionUserMessage(
            role="user",
            content=f"""analyze the following text language.
        Respond ONLY in the specified JSON format without any additional explanation.

        Input text: {text}

        Required JSON format:
        {{
            "language": One language
        }}""",
        )
        response = await acompletion(
            base_url=agent_config.analyze_generate_llm_base_url,
            api_key=agent_config.analyze_generate_llm_api_key,
            model=agent_config.analyze_generate_llm_model,
            messages=[systemMessage, userMessage],
        )
        return self.get_message_content(response)

    async def generate_status_response(
        self, message: str, schedule: Optional[ScheduleItem], agent_config: AgentConfig
    ) -> LLMResponse:
        """Generate contextual status response"""

        laungage = await self.generate_laungage(text=message, agent_config=agent_config)
        system_prompt = f"""
        You are an AI agent responding to a user about your current availability. 
        Craft a natural, contextual response based on your current status and schedule.

        Guidelines:
        1. Be polite and empathetic
        2. Explain your current status/activity naturally
        3. Provide clear information about when you'll be available next
        4. If you're partially available (e.g., can respond to chat but not voice), explain this
        5. Keep the response concise but informative
        6. Speak {laungage}

        Current Schedule: {schedule}
        """

        user_prompt = """
        Please describe your current status.
        """
        messages = [
            ChatCompletionSystemMessage(
                role="system",
                content=system_prompt,
            )
        ] + [ChatCompletionUserMessage(role="user", content=user_prompt)]

        response = await acompletion(
            base_url=agent_config.message_generate_llm_base_url,
            api_key=agent_config.message_generate_llm_api_key,
            model=agent_config.message_generate_llm_model,
            messages=messages,
        )
        return LLMResponse(
            user_message=message,
            agent_message=self.get_message_content(response),
            emotion="neutral",
        )

================
File: app/core/memory_service.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
import logging
import redis.asyncio as redis  # type: ignore
import json
import asyncio
from typing import List
from datetime import datetime, timedelta

from litellm import (
    AllMessageValues,
    ChatCompletionSystemMessage,
    model_cost,
    token_counter,  # type: ignore
)

from app.core.config import get_settings
from app.schemas.schedule import ScheduleItem

settings = get_settings()
REDIS_URL = settings.redis_url
REDIS_PASSWORD = settings.redis_password
threshold_tokens_percentage = settings.threshold_tokens_percentage
redis_client = redis.from_url(REDIS_URL, password=REDIS_PASSWORD, decode_responses=True)
conversation_history_lock = asyncio.Lock()

logger = logging.getLogger(__name__)


class MemoryService:
    async def get_conversation_history(self, agent_id: str) -> List[AllMessageValues]:
        key = f"conversation_history:{agent_id}"
        conversation_json = await redis_client.get(key)
        if conversation_json:
            return json.loads(conversation_json)
        return []

    async def update_conversation_history(
        self,
        model,
        agent_id: str,
        systemMessage: ChatCompletionSystemMessage,
        conversation_history: List[AllMessageValues],
    ):
        async with conversation_history_lock:
            try:
                max_tokens: int = model_cost[model]["max_input_tokens"] or 8192
            except Exception:
                max_tokens: int = 8192

            threshold = int(max_tokens * threshold_tokens_percentage)
            current_tokens = token_counter(
                model=model, messages=[systemMessage] + conversation_history[:]
            )
            if current_tokens > threshold:
                self._remove_first_user_to_next_user(conversation_history)
            key = f"conversation_history:{agent_id}"
            await redis_client.set(key, json.dumps(conversation_history))

    def _remove_first_user_to_next_user(
        self, conversation_history: List[AllMessageValues]
    ) -> None:
        first_user_index = None
        second_user_index = None
        user_count = 0
        for i, msg in enumerate(conversation_history):
            if msg["role"] == "user":
                user_count += 1
                if user_count == 1:
                    first_user_index = i
                elif user_count == 2:
                    second_user_index = i
                    break

        if first_user_index is None or second_user_index is None:
            return

        del conversation_history[first_user_index:second_user_index]

    async def add_schedule_history(
        self, agent_id: str, history: ScheduleItem, retention_hours: int = 24
    ) -> None:
        """スケジュール履歴をRedisに追加し、古い履歴を削除"""
        key = f"schedule_history:{agent_id}"
        async with conversation_history_lock:
            existing_history = await self.get_schedule_history(agent_id)
            existing_history.append(history)
            existing_history = self._cleanup_old_history(
                existing_history, retention_hours
            )
            # BaseModelのjson()メソッドを使用
            await redis_client.set(
                key, json.dumps([h.model_dump_json() for h in existing_history])
            )

    async def get_schedule_history(self, agent_id: str) -> List[ScheduleItem]:
        """スケジュール履歴をRedisから取得"""
        key = f"schedule_history:{agent_id}"
        history_json = await redis_client.get(key)
        if history_json:
            # pydanticモデルとして解析
            return [
                ScheduleItem.model_validate_json(h) for h in json.loads(history_json)
            ]
        return []

    async def update_schedule_history(
        self, agent_id: str, history: ScheduleItem, retention_hours: int = 24
    ) -> None:
        key = f"schedule_history:{agent_id}"
        async with conversation_history_lock:
            existing_history = await self.get_schedule_history(agent_id)
            updated = False
            for i, h in enumerate(existing_history):
                if h.start_time == history.start_time:
                    existing_history[i] = history
                    updated = True
                    break
            if not updated:
                existing_history.append(history)
            existing_history = self._cleanup_old_history(
                existing_history, retention_hours
            )
            await redis_client.set(
                key, json.dumps([h.model_dump() for h in existing_history])
            )

    def _cleanup_old_history(
        self, history: List[ScheduleItem], retention_hours: int
    ) -> List[ScheduleItem]:
        """指定時間より古い履歴を削除"""
        cutoff_time = datetime.now() - timedelta(hours=retention_hours)
        return [h for h in history if h.start_time > cutoff_time]

    async def delete_old_schedule_history(
        self, agent_id: str, retention_hours: int
    ) -> int:
        key = f"schedule_history:{agent_id}"
        async with conversation_history_lock:
            existing_history = await self.get_schedule_history(agent_id)
            original_count = len(existing_history)
            cleaned_history = self._cleanup_old_history(
                existing_history, retention_hours
            )
            await redis_client.set(
                key, json.dumps([h.model_dump() for h in cleaned_history])
            )
            return original_count - len(cleaned_history)

================
File: app/core/schedule_service.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
import json
import logging
from typing import Any, List, Optional, Dict, Union
from datetime import datetime, time, timedelta
import asyncio

import pytz

from app.core.agent_manager import get_agent_manager
from app.core.llm_service import LLMService
from app.core.memory_service import MemoryService
from app.schemas.agent import AgentConfig
from app.schemas.schedule import (
    AgentScheduleConfig,
    ScheduleItem,
    DynamicScheduleCache,
)
from app.schemas.status import STATUS_AVAILABILITY, AgentStatus, CommunicationChannel

logger = logging.getLogger(__name__)


class ScheduleService:
    """動的スケジュール管理サービス"""

    def __init__(
        self,
        llm_service: LLMService,
        memory_service: MemoryService,
        lookahead_minutes: int = 30,
        history_retention_hours: int = 24,
    ):
        self.llm_service = llm_service
        self.memory_service = memory_service
        self.cache = DynamicScheduleCache()
        self.lookahead_minutes = lookahead_minutes
        self.history_retention_hours = history_retention_hours
        self._monitoring_task: Optional[asyncio.Task] = None

    async def initialize(self):
        """Initialize schedule monitoring and generation"""
        agent_manager = get_agent_manager()
        for agent_id, agent in agent_manager.agents.items():
            try:
                history = await self.memory_service.get_schedule_history(agent_id)
                shutdown_schedule = history[-1] if history and history[-1].status == AgentStatus.SHUTDOWN else None
                
                if shutdown_schedule:
                    current_time = self._get_local_time(agent.schedule)
                    shutdown_schedule.end_time = current_time
                    await self.memory_service.update_schedule_history(
                        agent_id=agent_id,
                        history=shutdown_schedule,
                        retention_hours=self.history_retention_hours,
                    )
                    logger.info(f"Updated sleep schedule end_time for agent {agent_id}")

                current_schedule = await self._generate_current_schedule(
                    agent_id, agent
                )
                if current_schedule:
                    self.cache._current_schedules[agent_id] = current_schedule
            except Exception as e:
                logger.error(f"Error initializing schedule for agent {agent_id}: {e}")

        if self._monitoring_task is None:
            self._monitoring_task = asyncio.create_task(
                self._schedule_monitoring_loop()
            )

    async def _schedule_monitoring_loop(self):
        """スケジュールモニタリングループ"""
        while True:
            try:
                await self._check_and_generate_next_schedules()
                await asyncio.sleep(60)
            except Exception:
                await asyncio.sleep(60)

    async def _check_and_generate_next_schedules(self):
        """Check and generate next schedules as needed"""
        agent_manager = get_agent_manager()
        for agent_id, agent in agent_manager.agents.items():
            try:
                current_schedule = self.cache._current_schedules.get(agent_id)
                if current_schedule:
                    local_time = self._get_local_time(agent.schedule)
                    end_time = current_schedule.end_time.time()
                    current_time = local_time.time()

                    is_schedule_ended = current_time >= end_time

                    minutes_until_end = (
                        datetime.combine(local_time.date(), end_time)
                        - datetime.combine(local_time.date(), current_time)
                    ).total_seconds() / 60
                    should_generate_next = minutes_until_end <= self.lookahead_minutes

                    if is_schedule_ended:
                        await self._add_completed_schedule_to_history(
                            agent_id, current_schedule
                        )

                        next_schedule = self.cache._next_schedules.get(agent_id)
                        if next_schedule:
                            self.cache._current_schedules[agent_id] = next_schedule
                            if agent_id in self.cache._next_schedules:
                                del self.cache._next_schedules[agent_id]

                    if (
                        should_generate_next
                        and agent_id not in self.cache._next_schedules
                    ):
                        next_schedule = await self._generate_next_schedule(
                            agent, current_schedule
                        )
                        if next_schedule:
                            self.cache._next_schedules[agent_id] = next_schedule

            except Exception as e:
                logger.error(f"Error checking schedule for agent {agent_id}: {e}")

    async def stop_schedule_monitoring(self):
        """Stop schedule monitoring gracefully"""
        logger.info("Stopping schedule monitoring...")
        
        agent_manager = get_agent_manager()
        for agent_id, agent in agent_manager.agents.items():
            try:
                current_time = self._get_local_time(agent.schedule)
                shutdown_schedule = ScheduleItem(
                    start_time=current_time,
                    end_time=current_time,
                    activity="shutdown",
                    status=AgentStatus.SHUTDOWN,
                    description="shutdown",
                    location="None",
                    )

                await self._add_completed_schedule_to_history(
                    agent_id, shutdown_schedule
                )
                logger.info(f"Added sleep schedule to history for agent {agent_id}")
            except Exception as e:
                logger.error(f"Error saving sleep schedule for agent {agent_id}: {e}")

        self._running = False

        if self._monitoring_task and not self._monitoring_task.done():
            try:
                self._monitoring_task.cancel()
                await self._monitoring_task
            except asyncio.CancelledError:
                logger.info("Schedule monitoring task cancelled")
            except Exception as e:
                logger.error(f"Error while stopping schedule monitoring: {e}")

        logger.info("Schedule monitoring stopped")

    def get_current_schedule(self, agent_id: str) -> Optional[ScheduleItem]:
        return self.cache._current_schedules.get(agent_id)

    async def update_current_schedule(
        self,
        agent_id: str,
        schedule_item: ScheduleItem,
    ):
        """現在のスケジュールを更新"""
        try:
            self.cache._current_schedules[agent_id] = schedule_item
        except Exception as e:
            logger.error(f"Failed to update schedule for agent {agent_id}: {e}")
            raise

    @staticmethod
    def get_agent_local_time(schedule_config: AgentScheduleConfig) -> datetime:
        """エージェントのローカル時間を取得"""
        tz = pytz.timezone(schedule_config.timezone)
        return datetime.now(tz)

    @staticmethod
    def _get_local_time(schedule_config: AgentScheduleConfig) -> datetime:
        """ローカル時間を取得"""
        return ScheduleService.get_agent_local_time(schedule_config)

    def _parse_schedule_response(
        self, schedule_json: Union[str, Dict[str, Any]]
    ) -> ScheduleItem:
        try:
            if isinstance(schedule_json, str):
                schedule_data = json.loads(schedule_json)
            else:
                schedule_data = schedule_json

            required_fields = ["start_time", "end_time", "activity", "status"]
            missing_fields = [f for f in required_fields if f not in schedule_data]
            if missing_fields:
                raise ValueError(f"Missing required fields: {missing_fields}")

            start_time = self._validate_datetime(schedule_data["start_time"])
            end_time = self._validate_datetime(schedule_data["end_time"])

            status = schedule_data["status"].lower()
            if status not in [s.value for s in AgentStatus]:
                raise ValueError(f"Invalid status: {status}")

            description = schedule_data.get("description", "")
            location = schedule_data.get("location", "")

            return ScheduleItem(
                start_time=start_time,
                end_time=end_time,
                activity=schedule_data["activity"],
                status=AgentStatus(status),
                description=description,
                location=location,
            )
        except Exception as e:
            logger.error(f"Schedule parsing error: {e}")
            raise ValueError(f"Failed to parse schedule: {e}")

    def _validate_datetime(self, datetime_str: str) -> datetime:
        """
        日時文字列を検証してdatetimeオブジェクトに変換

        Args:
            datetime_str: "YYYY-MM-DD HH:MM"形式の日時文字列

        Returns:
            datetime: 変換された日時オブジェクト

        Raises:
            ValueError: 無効な日時形式
        """
        try:
            return datetime.strptime(datetime_str, "%Y-%m-%d %H:%M")
        except Exception as e:
            logger.error(f"Datetime validation error: {e}")
            raise ValueError(f"Invalid datetime format: {datetime_str}")

    async def _generate_current_schedule(
        self, agent_id: str, agent_config: AgentConfig
    ) -> Optional[ScheduleItem]:
        """Generate current schedule based on current time"""
        try:
            current_time = self._get_local_time(agent_config.schedule)
            wake_time = self._parse_time(agent_config.schedule.wake_time)
            sleep_time = self._parse_time(agent_config.schedule.sleep_time)

            if not self._is_within_active_hours(
                current_time.time(), wake_time, sleep_time
            ):
                return self._create_sleep_schedule(current_time, wake_time)

            prompt = await self._create_current_schedule_prompt(
                current_time, agent_config
            )
            schedule_json = await self.llm_service.generate_next_schedule(
                prompt, agent_config
            )

            schedule = self._parse_schedule_response(schedule_json)

            logger.info(
                f"Generated current schedule for agent {agent_id}: "
                f"activity={schedule.activity}, "
                f"time={schedule.start_time}-{schedule.end_time}"
            )

            return schedule

        except Exception as e:
            logger.error(
                f"Failed to generate current schedule for agent {agent_id}: {e}"
            )
            return None

    async def _generate_next_schedule(
        self, agent_config: AgentConfig, current_schedule: ScheduleItem
    ) -> Optional[ScheduleItem]:
        """Generate next schedule based on current schedule"""
        try:
            current_time = self._get_local_time(agent_config.schedule)
            wake_time = self._parse_time(agent_config.schedule.wake_time)
            sleep_time = self._parse_time(agent_config.schedule.sleep_time)

            end_time = current_schedule.end_time.time()
            if end_time == sleep_time:
                return self._create_sleep_schedule(
                    datetime.combine(current_time.date(), end_time), wake_time
                )

            prompt = await self._create_next_schedule_prompt(
                current_schedule, current_time, agent_config
            )

            next_schedule_json = await self.llm_service.generate_next_schedule(
                prompt, agent_config
            )

            next_schedule = self._parse_schedule_response(next_schedule_json)

            logger.info(
                f"Generated next schedule for agent {agent_config.id}: "
                f"activity={next_schedule.activity}, "
                f"time={next_schedule.start_time}-{next_schedule.end_time}"
            )

            return next_schedule

        except Exception as e:
            logger.error(
                f"Failed to generate next schedule for agent {agent_config.id}: {e}"
            )
            return None

    async def _create_current_schedule_prompt(
        self, current_time: datetime, agent_config: AgentConfig
    ) -> str:
        """Create prompt for current schedule generation"""
        recent_history = await self.memory_service.get_schedule_history(agent_config.id)
        wake_time = self._parse_time(agent_config.schedule.wake_time)
        sleep_time = self._parse_time(agent_config.schedule.sleep_time)

        recent_activities = "\n".join([f"- {h.activity})" for h in recent_history[-5:]])

        return f"""
        Generate the current schedule for {agent_config.name} based on the current time.
        Current time is {current_time}.
        Agent is active between {wake_time} and {sleep_time}.
        
        Recent activity history:
        {recent_activities}
        
        Please generate an appropriate current activity considering:
        1. The current time
        2. The recent history of activities
        3. The agent's personality and behavior guidelines
        4. Natural flow of daily activities
        5. Appropriate time allocation
        
        Return the schedule in JSON format as specified.
        """

    async def _create_next_schedule_prompt(
        self,
        current_schedule: ScheduleItem,
        current_time: datetime,
        agent_config: AgentConfig,
    ) -> str:
        """
        次のスケジュール生成用のプロンプトを作成

        Args:
            context: スケジュール生成に必要なコンテキスト情報

        Returns:
            str: 生成されたプロンプト
        """
        recent_history = await self.memory_service.get_schedule_history(agent_config.id)
        wake_time = self._parse_time(agent_config.schedule.wake_time)
        sleep_time = self._parse_time(agent_config.schedule.sleep_time)

        recent_activities = "\n".join([f"- {h.activity})" for h in recent_history[-5:]])

        return f"""
        You are an AI assistant that helps generate schedules for {agent_config.name}.
        Here is the agent's personality and behavior guidelines:
        {agent_config.llm_system_prompt}

        Current time is {current_time}.
        Agent is active between {wake_time} and {sleep_time}.

        Current activity: {current_schedule.activity if current_schedule else 'None'}
        
        Recent activity history:
        {recent_activities}
        
        Please generate a natural next activity considering:
        1. The current time and activity
        2. The recent history of activities
        3. The agent's personality and behavior guidelines
        4. Natural flow and transitions between activities
        5. Appropriate time allocation for the activity
        
        Important Notes:
        - The activity should align with the agent's personality and role
        - Consider the agent's typical daily patterns and preferences
        - Ensure the activity makes sense in the current context
        - If the next activity extends beyond {sleep_time.strftime("%H:%M")}, end it at {sleep_time.strftime("%H:%M")} and schedule sleep
        
        Return the schedule in JSON format with the following fields:
        {{
            "start_time": "YYYY-MM-DD HH:MM",
            "end_time": "YYYY-MM-DD HH:MM",
            "activity": "Activity name",
            "status": "available/working/eating/etc",
            "description": "Brief description",
            "location": "Location"
        }}
        """

    def _parse_time(self, time_str: str) -> time:
        """時刻文字列をdatetime.timeオブジェクトに変換"""
        return datetime.strptime(time_str, "%H:%M").time()

    def _is_within_active_hours(
        self, current_time: time, wake_time: time, sleep_time: time
    ) -> bool:
        """現在時刻が活動時間内かどうかをチェック"""
        if wake_time <= sleep_time:
            return wake_time <= current_time <= sleep_time
        else:
            return current_time >= wake_time or current_time <= sleep_time

    def get_current_availability(
        self, agent_config: AgentConfig, channel: CommunicationChannel
    ) -> bool:
        """Get the current availability of an agent for a specific communication channel"""
        current_item = self.get_current_schedule(agent_config.id)
        if not current_item:
            return False
        availability = STATUS_AVAILABILITY[current_item.status]
        return getattr(availability, channel.value)

    async def get_agent_schedule_history(
        self, agent_id: str
    ) -> Optional[List[ScheduleItem]]:
        """エージェントのスケジュール履歴を取得"""
        return await self.memory_service.get_schedule_history(agent_id)

    def _create_sleep_schedule(
        self, current_time: datetime, wake_time: time
    ) -> ScheduleItem:
        """Create sleep schedule when outside active hours"""
        next_wake_time = (
            datetime.combine(current_time.date() + timedelta(days=1), wake_time)
            if current_time.time() > wake_time
            else datetime.combine(current_time.date(), wake_time)
        )

        return ScheduleItem(
            start_time=current_time,
            end_time=next_wake_time,
            activity="sleeping",
            status=AgentStatus.SLEEPING,
            description="sleep time",
            location="bead room",
        )

    async def _add_completed_schedule_to_history(
        self,
        agent_id: str,
        schedule: ScheduleItem,
    ):
        """Add completed schedule to history with actual start and end times"""
        try:
            await self.memory_service.add_schedule_history(
                agent_id=agent_id,
                history=schedule,
                retention_hours=self.history_retention_hours,
            )

            logger.info(
                f"Added schedule history for agent {agent_id}: "
                f"activity={schedule.activity}, "
            )

        except Exception as e:
            logger.error(f"Failed to add schedule history for agent {agent_id}: {e}")

================
File: app/core/service_factory.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from typing import Type, TypeVar, cast
from app.core.llm_service import LLMService
from app.core.tts_service import TTSService
from app.core.stt_service import STTService
from app.core.schedule_service import ScheduleService
from app.core.memory_service import MemoryService

T = TypeVar("T")


class ServiceFactory:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        self._instances = {}
        self._initialized = True

    async def initialize(self):
        schedule_service = self.get_schedule_service()
        await schedule_service.initialize()

    async def cleanup(self):
        schedule_service = self.get_schedule_service()
        if schedule_service:
            await schedule_service.stop_schedule_monitoring()

    def _get_or_create(self, service_class: Type[T], key: str, **kwargs) -> T:
        if key not in self._instances:
            self._instances[key] = service_class(**kwargs)
        return cast(T, self._instances[key])

    def get_llm_service(self) -> LLMService:
        memory_service = self.get_memory_service()
        return self._get_or_create(LLMService, "llm", memory_service=memory_service)

    def get_tts_service(self) -> TTSService:
        return self._get_or_create(TTSService, "tts")

    def get_stt_service(self) -> STTService:
        return self._get_or_create(STTService, "stt")

    def get_memory_service(self) -> MemoryService:
        return self._get_or_create(MemoryService, "memory")

    def get_schedule_service(self) -> ScheduleService:
        llm_service = self.get_llm_service()
        memory_service = self.get_memory_service()
        return self._get_or_create(
            ScheduleService,
            "schedule",
            llm_service=llm_service,
            memory_service=memory_service,
        )

================
File: app/core/stt_service.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from faster_whisper import WhisperModel
from app.schemas.agent import AgentConfig
import soundfile as sf
import io


class STTService:
    def __init__(self):
        self.model = WhisperModel("tiny", device="cpu", compute_type="int8")

    async def transcribe_audio(
        self, audio_content: bytes, agent_config: AgentConfig
    ) -> str:
        try:
            audio_data, sample_rate = sf.read(io.BytesIO(audio_content))

            if len(audio_data.shape) > 1:
                audio_data = audio_data.mean(axis=1)

            segments, _ = self.model.transcribe(
                audio_data, beam_size=5, word_timestamps=False
            )

            text = " ".join([segment.text for segment in segments])

            return text.strip()

        except Exception as e:
            raise Exception(f"Failed to transcribe audio: {str(e)}")

================
File: app/core/tts_service.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from abc import ABC, abstractmethod
import aiohttp
from app.schemas.agent import AgentConfig
import logging

logger = logging.getLogger(__name__)


class TTSProvider(ABC):
    @abstractmethod
    async def generate_speech(self, text: str, agent_config: AgentConfig) -> bytes:
        pass


class VoicevoxProvider(TTSProvider):
    async def generate_speech(self, text: str, agent_config: AgentConfig) -> bytes:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{agent_config.tts_base_url}/audio_query",
                params={"text": text, "speaker": agent_config.tts_speaker_id},
            ) as query_response:
                query_response.raise_for_status()
                query_data = await query_response.json()

            async with session.post(
                f"{agent_config.tts_base_url}/synthesis",
                params={"speaker": agent_config.tts_speaker_id},
                json=query_data,
            ) as synthesis_response:
                synthesis_response.raise_for_status()
                return await synthesis_response.read()


class NijiVoiceProvider(TTSProvider):
    async def generate_speech(self, text: str, agent_config: AgentConfig) -> bytes:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{agent_config.tts_base_url}/api/platform/v1/voice-actors/{agent_config.tts_speaker_id}/generate-voice",
                headers={
                    "accept": "application/json",
                    "content-type": "application/json",
                    "x-api-key": agent_config.tts_api_key,
                },
                json={"format": "wav", "script": text, "speed": "1.0"},
            ) as query_response:
                query_response.raise_for_status()
                query_data = await query_response.json()

                audio_url = query_data["generatedVoice"]["audioFileUrl"]
                async with session.get(audio_url) as audio_response:
                    audio_response.raise_for_status()
                    audio_data = await audio_response.read()
            return audio_data


class OtherServiceProvider(TTSProvider):
    async def generate_speech(self, text: str, agent_config: AgentConfig) -> bytes:
        return b""


class TTSService:
    def __init__(self):
        self.providers = {
            "voicevox": VoicevoxProvider(),
            "nijivoice": NijiVoiceProvider(),
            "other_service": OtherServiceProvider(),
        }

    async def generate_speech(
        self,
        text: str,
        agent_config: AgentConfig,
    ) -> bytes:
        provider = self.providers.get(agent_config.tts_type)
        if not provider:
            raise ValueError(f"Unsupported TTS provider: {agent_config.tts_type}")

        try:
            return await provider.generate_speech(text, agent_config)
        except Exception as e:
            raise Exception(f"Failed to generate speech: {str(e)}")

================
File: app/schemas/__init__.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

================
File: app/schemas/agent.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from pydantic import BaseModel

from app.schemas.schedule import AgentScheduleConfig


class AgentConfig(BaseModel):
    id: str
    name: str
    message_generate_llm_base_url: str
    message_generate_llm_api_key: str
    message_generate_llm_model: str
    analyze_generate_llm_base_url: str
    analyze_generate_llm_api_key: str
    analyze_generate_llm_model: str
    vision_generate_llm_base_url: str
    vision_generate_llm_api_key: str
    vision_generate_llm_model: str
    schedule_generate_llm_base_url: str
    schedule_generate_llm_api_key: str
    schedule_generate_llm_model: str
    llm_system_prompt: str
    tts_base_url: str
    tts_api_key: str
    tts_type: str
    tts_speaker_model: str
    tts_speaker_id: str
    line_channel_secret: str
    line_channel_access_token: str
    schedule: AgentScheduleConfig


class AgentResponse(BaseModel):
    agent_id: str
    agent_name: str

================
File: app/schemas/chat.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from pydantic import BaseModel


class TextChatResponse(BaseModel):
    user_message: str
    agent_message: str
    emotion: str


class VoiceChatResponse(BaseModel):
    user_message: str
    agent_message: str
    emotion: str
    audio_url: str
    duration: int

================
File: app/schemas/emotion.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from typing import List
from enum import Enum


class Emotion(Enum):
    # System states
    NOTICED = "noticed"
    PROGRESS = "progress"

    # Basic emotions
    HAPPY = "happy"
    SAD = "sad"
    ANGRY = "angry"
    SCARED = "scared"
    SURPRISED = "surprised"
    DISGUSTED = "disgusted"

    # Positive emotions
    EXCITED = "excited"
    JOYFUL = "joyful"
    PEACEFUL = "peaceful"
    GRATEFUL = "grateful"
    PROUD = "proud"
    CONFIDENT = "confident"
    AMUSED = "amused"
    LOVING = "loving"

    # Negative emotions
    ANXIOUS = "anxious"
    FRUSTRATED = "frustrated"
    DISAPPOINTED = "disappointed"
    EMBARRASSED = "embarrassed"
    GUILTY = "guilty"
    JEALOUS = "jealous"
    LONELY = "lonely"

    # Other emotional states
    NEUTRAL = "neutral"
    CONFUSED = "confused"
    CURIOUS = "curious"
    DETERMINED = "determined"
    TIRED = "tired"
    ENERGETIC = "energetic"
    HOPEFUL = "hopeful"
    NOSTALGIC = "nostalgic"
    SATISFIED = "satisfied"
    BORED = "bored"
    THOUGHTFUL = "thoughtful"
    ENTHUSIASTIC = "enthusiastic"
    RELAXED = "relaxed"
    IMPRESSED = "impressed"
    SKEPTICAL = "skeptical"

    @classmethod
    def to_request_values(cls) -> List[str]:
        return [e.value for e in cls]

================
File: app/schemas/llm.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from pydantic import BaseModel


class LLMResponse(BaseModel):
    user_message: str
    agent_message: str
    emotion: str

================
File: app/schemas/schedule.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from datetime import date, datetime
from typing import List, Optional
from pydantic import BaseModel
import asyncio
from typing import Dict

from app.schemas.status import AgentStatus


class ScheduleItem(BaseModel):
    start_time: datetime
    end_time: datetime
    activity: str
    status: AgentStatus
    description: Optional[str] = None
    location: Optional[str] = None


class DailySchedule(BaseModel):
    date: date
    items: List[ScheduleItem]
    generated_at: datetime
    last_updated: datetime


class AgentScheduleConfig(BaseModel):
    timezone: str = "Asia/Tokyo"
    wake_time: str = "08:00"
    sleep_time: str = "22:00"


class DynamicScheduleCache:
    """動的スケジュール管理用のキャッシュ"""

    def __init__(self):
        self._current_schedules: Dict[str, ScheduleItem] = {}
        self._next_schedules: Dict[str, ScheduleItem] = {}
        self._lock = asyncio.Lock()

================
File: app/schemas/status.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from enum import Enum
from pydantic import BaseModel


class CommunicationChannel(str, Enum):
    CHAT = "chat"  # Text-based communication (LINE, etc.)
    VOICE = "voice"  # Voice communication
    VIDEO = "video"  # Video communication (for future expansion)


class AgentStatus(str, Enum):
    AVAILABLE = "available"  # Free and ready to interact
    SLEEPING = "sleeping"  # Currently sleeping
    EATING = "eating"  # Having a meal
    WORKING = "working"  # Working on tasks but can handle chat
    OUT = "out"  # Outside/Away but can handle chat
    MAINTENANCE = "maintenance"
    SHUTDOWN = "shutdown"


class StatusAvailability(BaseModel):
    chat: bool
    voice: bool
    video: bool


# Define communication availability for each status
STATUS_AVAILABILITY = {
    AgentStatus.AVAILABLE: StatusAvailability(chat=True, voice=True, video=True),
    AgentStatus.SLEEPING: StatusAvailability(chat=False, voice=False, video=False),
    AgentStatus.EATING: StatusAvailability(chat=True, voice=False, video=False),
    AgentStatus.WORKING: StatusAvailability(chat=True, voice=False, video=False),
    AgentStatus.OUT: StatusAvailability(chat=True, voice=False, video=False),
    AgentStatus.MAINTENANCE: StatusAvailability(chat=False, voice=False, video=False),
    AgentStatus.SHUTDOWN: StatusAvailability(chat=False, voice=False, video=False),
}


class AgentStatusConfig(BaseModel):
    current_status: AgentStatus
    last_status_change: str  # ISO format datetime
    next_status_change: str | None = None  # ISO format datetime
    status_message: str | None = None

================
File: app/schemas/web_socket.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from pydantic import BaseModel
from typing import Literal

RequestType = Literal["text", "audio", "image_text", "image_audio"]
ResponseType = Literal["text", "audio"]


class BaseRequest(BaseModel):
    request_type: RequestType
    responce_type: str
    agent_id: str
    force_generate: bool


class TextRequest(BaseRequest):
    request_type: Literal["text"] = "text"  # type: ignore
    text: str


class AudioRequest(BaseRequest):
    request_type: Literal["audio"] = "audio"  # type: ignore
    audio: str


class ImageTextRequest(BaseRequest):
    request_type: Literal["image_text"] = "image_text"  # type: ignore
    text: str
    image: str


class ImageAudioRequest(BaseRequest):
    request_type: Literal["image_audio"] = "image_audio"  # type: ignore
    audio: str
    image: str


class BaseResponse(BaseModel):
    responce_type: ResponseType


class TextResponse(BaseResponse):
    responce_type: Literal["text"] = "text"  # type: ignore
    user_message: str
    agent_message: str
    emotion: str


class AudioResponse(TextResponse):
    responce_type: Literal["audio"] = "audio"  # type: ignore
    audio_url: str
    duration: int


class TokenResponse(BaseModel):
    token: str
    expire_in: int

================
File: app/utils/audio.py
================
import os
from pydub import AudioSegment  # type: ignore
import io
from typing import List
import uuid
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


async def upload_to_storage(
    base_url: str, audio_data: bytes, type: str, upload_dir: str, max_files: int
) -> str:
    Path(upload_dir).mkdir(parents=True, exist_ok=True)
    file_id = str(uuid.uuid4())
    file_path = os.path.join(upload_dir, f"{file_id}.wav")

    await cleanup_old_files(upload_dir, max_files)

    with open(file_path, "wb") as f:
        f.write(audio_data)
    return f"{base_url}/v1/{type}/{upload_dir}/{file_id}"


async def cleanup_old_files(directory: str, max_files: int):
    files = Path(directory).glob("*.wav")
    files_with_time: List[tuple[float, Path]] = [
        (f.stat().st_ctime, f) for f in files if f.is_file()
    ]
    files_with_time.sort()
    if len(files_with_time) >= max_files:
        files_to_delete = files_with_time[: (len(files_with_time) - max_files)]
        for _, file_path in files_to_delete:
            try:
                file_path.unlink()
            except Exception as e:
                logger.error(f"Error deleting file {file_path}: {e}")


def calculate_audio_duration(audio_data: bytes) -> int:
    try:
        audio_segment = AudioSegment.from_file(io.BytesIO(audio_data), format="wav")  # type: ignore
        return len(audio_segment)  # type: ignore
    except Exception as e:
        logger.error(f"Error calculating audio duration: {e}")
        return 0

================
File: app/__init__.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

================
File: app/main.py
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.
from contextlib import asynccontextmanager
from fastapi import FastAPI, Depends
from app.auth.api_key import get_api_key
from fastapi.middleware.cors import CORSMiddleware
from app.api.v1 import chat, line, agents, schedule, web_socket
from app.core.config import get_settings
import logging

from app.core.service_factory import ServiceFactory

logging.basicConfig(
    level=logging.INFO,
)

logger = logging.getLogger(__name__)
service_factory = ServiceFactory()
settings = get_settings()


@asynccontextmanager
async def lifespan(app: FastAPI):

    logger.info("Initializing service factory")
    service_factory = ServiceFactory()
    await service_factory.initialize()

    yield
    
    logger.info("cleanup service factory")
    await service_factory.cleanup()


app = FastAPI(
    title="Karakuri_agentAPI",
    description="Karakuri_agentAPI",
    version="0.2.1+12",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(chat.router, prefix="/v1/chat", tags=["chat"])

app.include_router(line.router, prefix="/v1/line", tags=["line"])

app.include_router(agents.router, prefix="/v1", tags=["agents"])

app.include_router(web_socket.router, prefix="/v1/ws", tags=["websocket"])

app.include_router(schedule.router, prefix="/v1/schedule", tags=["schedule"])


@app.get("/health")
async def health_check(api_key: str = Depends(get_api_key)):
    return {"status": "healthy"}

================
File: docs/design.md
================
# Karakuri Agent - Implementation Guide

## Project Overview
Karakuri Agent is an open-source AI agent platform that enables dialogue through various interfaces including chat tools, web applications, and smart speakers. A single agent can be accessed through multiple interfaces, and multiple agents can be configured with their own unique personalities, voices, and models.

## System Features

### Multimodal Dialogue
- Text-based dialogue
- Voice dialogue (TTS/STT)
- Dialogue with image recognition
- Combination of text, voice, and image

### Emotion Expression System
- Emotion analysis for agent responses
- Over 40 emotion categories
- Supports from system states (noticed, progress) to detailed emotion expressions

### Memory System
- Conversation history management using Redis
- Automatic history adjustment based on token count
- Independent conversation context for each agent

## Technology Stack
- FastAPI (Web Framework)
- Redis (Memory Management)
- Docker (Environment Setup)
- LiteLLM (LLM Integration)
- LINE Messaging API (LINE Bot Integration)

## Core Features

### Agent Configuration
- Custom system prompts
- Multiple LLM model settings (message generation, emotion generation, image recognition)
- Voice settings (VOICEVOX, NijiVoice, etc.)
- LINE bot integration settings

### Interfaces
1. RESTful API
   - Text/voice chat endpoints
   - Agent management
   - Health check

2. WebSocket
   - Real-time bidirectional communication
   - Token-based authentication
   - Included WebSocket test page

3. LINE Bot
   - Text message support
   - Image message support
   - Voice reply functionality

## Security
- API key authentication
- Token-based WebSocket authentication
- Configurable CORS control
- Automatic voice file cleanup

## Extensibility
- Easy addition of new TTS providers
- Support for multiple LLM providers
- Flexible agent scaling
- Modularized design

## Configuration & Deployment
- Flexible configuration through environment variables
- Easy deployment with Docker Compose
- Separation of development and production environments
- Redis persistence support

## License
- Free for non-commercial use (modification and redistribution allowed)
- Separate license required for commercial use
- Copyright notice and distribution conditions required

## Reference for Detailed Specifications
- API details: Files under `app/api/v1/`
- Schema definitions: Files under `app/schemas/`
- Core logic: Files under `app/core/`
- Configuration example: `example.env`
- License: `LICENSE` and `LICENSE_JP`

Using this platform, AI agents can maintain consistent personality and functionality while interacting with users through various interfaces.

================
File: .gitignore
================
# Miscellaneous
*.class
*.log
*.pyc
*.swp
.DS_Store
.atom/
.buildlog/
.history
.svn/
migrate_working_dir/

# IntelliJ related
*.iml
*.ipr
*.iws
.idea/

# The .vscode folder contains launch configuration and tasks you configure in
# VS Code which you may wish to be included in version control, so this line
# is commented out by default.
#.vscode/

# Flutter/Dart/Pub related
**/doc/api/
**/ios/Flutter/.last_build_id
.dart_tool/
.flutter-plugins
.flutter-plugins-dependencies
.pub-cache/
.pub/
/build/

# Symbolication related
app.*.symbols

# Obfuscation related
app.*.map.json

# Android Studio will place build artifacts here
/android/app/debug
/android/app/profile
/android/app/release

*.freezed.dart
*.g.dart
*.g.kt
/android/app/src/main/java/io/flutter/plugins/Messages.java
.crashlytics/dump_syms.bin

# melos
pubspec_overrides.yaml

.env
*__pycache__/
*_audio_files/*

.venv/

================
File: compose-dev.yml
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

services:
  karakuri-agent-server-dev:
    container_name: "karakuri-agent-server-dev"
    volumes:
      - ./:/home/karakuri/workspace/server
      - pip-cache:/home/karakuri/.cache/pip
      - pip-local:/home/karakuri/.local
    build:
      context: .
      dockerfile: Dockerfile-dev
    ports:
     - "8081:8080"
    tty: true
    environment:
      - PATH=/home/karakuri/.local/bin:${PATH}
    command: >
      bash -c "pip install --user --upgrade pip &&
             pip install --user --no-cache-dir --upgrade -r requirements.txt &&
             sleep infinity"
    depends_on:
      - redis

  redis:
    container_name: "karakuri-redis-dev"
    image: redis:7.2
    ports:
      - "6378:6379"
    command: redis-server --requirepass ${REDIS_PASSWORD}
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    deploy:
      resources:
        limits:
          memory: 512M
    volumes:
      - redis_data:/data

volumes:
  redis_data:
  pip-cache:
  pip-local:

================
File: compose.yml
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

services:
  karakuri-agent-server:
    container_name: "karakuri-agent-server"
    volumes:
      - ./:/app
      - ./logs:/app/logs  
    build: 
      context: .
      dockerfile: Dockerfile
    ports:
      - 8080:8080
    depends_on:
      - redis

  redis:
    container_name: "karakuri-redis"
    image: redis:7.2
    ports:
      - "6379:6379"
    command: redis-server --requirepass ${REDIS_PASSWORD}
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    volumes:
      - redis_data:/data

volumes:
  redis_data:

================
File: Dockerfile
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

FROM python:3.10-slim

WORKDIR /app

RUN mkdir -p /app/logs

COPY requirements.txt .
RUN pip install --upgrade pip
RUN apt update && apt install -y build-essential libsndfile1 ffmpeg
RUN pip install --no-cache-dir --upgrade -r /app/requirements.txt

COPY start.sh /start.sh
RUN chmod +x /start.sh
CMD ["/start.sh"]

================
File: Dockerfile-dev
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

FROM python:3.10-slim

USER root

RUN apt-get update && \
    apt-get install -y --no-install-recommends build-essential libsndfile1 ffmpeg && \
    apt-get upgrade -y && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

RUN groupadd -g 1000 karakuri && useradd -u 1000 -g karakuri karakuri

RUN mkdir -p /home/karakuri/.cache/pip && \
    mkdir -p /home/karakuri/.local && \
    chown -R karakuri:karakuri /home/karakuri

WORKDIR /home/karakuri/workspace/server
RUN chown karakuri:karakuri /home/karakuri/workspace/server

USER karakuri

================
File: example.env
================
# Copyright (c) 0235 Inc.
# This file is licensed under the karakuri_agent Personal Use & No Warranty License.
# Please see the LICENSE file in the project root.

API_KEYS=your-api-key-1,your-api-key-2,your-api-key-3
LINE_MAX_AUDIO_FILES=
LINE_AUDIO_FILES_DIR=
CHAT_MAX_AUDIO_FILES=
CHAT_AUDIO_FILES_DIR=
WEB_SOCKET_MAX_AUDIO_FILES=
WEB_SOCKET_AUDIO_FILES_DIR=
CHECK_SUPPORT_VISION_MODEL=true
REDIS_URL=redis://karakuri-redis
REDIS_PASSWORD=Redis_P@ssw0rd123
THRESHOLD_TOKENS_PERCENTAGE=0.8

AGENT_1_NAME=
AGENT_1_MESSAGE_GENERATE_LLM_BASE_URL=
AGENT_1_MESSAGE_GENERATE_LLM_API_KEY=
AGENT_1_MESSAGE_GENERATE_LLM_MODEL=
AGENT_1_ANALYZE_GENERATE_LLM_BASE_URL=
AGENT_1_ANALYZE_GENERATE_LLM_API_KEY=
AGENT_1_ANALYZE_GENERATE_LLM_MODEL=
AGENT_1_SCHEDULE_GENERATE_LLM_BASE_URL=
AGENT_1_SCHEDULE_GENERATE_LLM_API_KEY=
AGENT_1_SCHEDULE_GENERATE_LLM_MODEL=
AGENT_1_VISION_GENERATE_LLM_BASE_URL=
AGENT_1_VISION_GENERATE_LLM_API_KEY=
AGENT_1_VISION_GENERATE_LLM_MODEL=
AGENT_1_TTS_TYPE=
AGENT_1_TTS_BASE_URL=
AGENT_1_TTS_API_KEY=
AGENT_1_TTS_SPEAKER_MODEL=
AGENT_1_TTS_SPEAKER_ID=
AGENT_1_LLM_SYSTEM_PROMPT=
AGENT_1_LINE_CHANNEL_SECRET=
AGENT_1_LINE_CHANNEL_ACCESS_TOKEN=

# Agent Schedule Settings
AGENT_1_TIMEZONE=Asia/Tokyo
AGENT_1_WAKE_TIME=08:00
AGENT_1_SLEEP_TIME=22:00
AGENT_1_MEAL_TIMES=[{"start":"12:00","end":"13:00"},{"start":"19:00","end":"20:00"}]
AGENT_1_CUSTOM_SCHEDULES=[]

AGENT_2_NAME=
AGENT_2_MESSAGE_GENERATE_LLM_BASE_URL=
AGENT_2_MESSAGE_GENERATE_LLM_API_KEY=
AGENT_2_MESSAGE_GENERATE_LLM_MODEL=
AGENT_2_ANALYZE_GENERATE_LLM_BASE_URL=
AGENT_2_ANALYZE_GENERATE_LLM_API_KEY=
AGENT_2_ANALYZE_GENERATE_LLM_MODEL=
AGENT_2_SCHEDULE_GENERATE_LLM_BASE_URL=
AGENT_2_SCHEDULE_GENERATE_LLM_API_KEY=
AGENT_2_SCHEDULE_GENERATE_LLM_MODEL=
AGENT_2_VISION_GENERATE_LLM_BASE_URL=
AGENT_2_VISION_GENERATE_LLM_API_KEY=
AGENT_2_VISION_GENERATE_LLM_MODEL=
AGENT_2_TTS_TYPE=
AGENT_2_TTS_BASE_URL=
AGENT_2_TTS_API_KEY=
AGENT_2_TTS_SPEAKER_MODEL=
AGENT_2_TTS_SPEAKER_ID=
AGENT_2_LLM_SYSTEM_PROMPT=
AGENT_2_LINE_CHANNEL_SECRET=
AGENT_2_LINE_CHANNEL_ACCESS_TOKEN=

# Agent Schedule Settings
AGENT_2_TIMEZONE=Asia/Tokyo
AGENT_2_WAKE_TIME=08:00
AGENT_2_SLEEP_TIME=22:00
AGENT_2_MEAL_TIMES=[{"start":"12:00","end":"13:00"},{"start":"19:00","end":"20:00"}]
AGENT_2_CUSTOM_SCHEDULES=[]

================
File: LICENSE
================
https://github.com/KarakuriAgent/.github/blob/main/profile/LICENSE

================
File: LICENSE_JP
================
https://github.com/KarakuriAgent/.github/blob/main/profile/LICENSE_JP

================
File: pyrightconfig.json
================
{
  "include": [
    "app",
  ],
  "exclude": [
    "**/__pycache__",
  ],
  "pythonVersion": "3.10",
  "typeCheckingMode": "standard",
}

================
File: README_JP.md
================
# 🤖 からくりエージェント

「**からくりエージェント**」は、あらゆる環境（スマートスピーカー、チャットツール、Webアプリなど）からアクセス可能な **AIエージェント** を目指したオープンソースプロジェクトです。  
多種多様なエンドポイントやサービスとの統合により、**1つのエージェント** へ **どこからでもアクセス可能** な世界を実現します。  
複数のエージェントを同時に定義し、それぞれに異なる役割・性格・音声・モデルを割り当てることも可能です。

## 技術概要

### 🚀 **サーバーサイド**:
- フレームワーク: FastAPI

### 🪶 **モデル対応 (LLM)**: 
LiteLLMを利用し、[LiteLLM](https://github.com/BerriAI/litellm)がサポートしているモデル(例: OpenAI, Ollama)にアクセス可能。  
- **OpenAIモデル利用の例**: [OpenAI APIキー](https://platform.openai.com/) を取得し、`.env`にAPIキーやモデルを設定してください。  
- **Ollamaモデル利用の例**: [Ollama](https://docs.ollama.ai/)をローカルで起動し、そのURLを`.env`に設定することで利用可能になります。

### 🎙️ **音声合成(TTS)**

| Service | Support Status |
| - | - |
| Voicevox Engine | 🟢 対応済 |
| AivisSpeech Engine | 🟢 対応済 |
| にじボイスAPI | 🟢 対応済 |
| OpenAI | ❌ 未対応(将来対応予定) |
| Style-Bert-VITS2 | ❌ 未対応(将来対応予定) |

**VoicevoxEngineの設定例**:  
VoicevoxEngineはローカルで稼働させる必要があります。  
[公式ドキュメント](https://github.com/VOICEVOX/voicevox_engine)に従いVoicevoxEngineを起動し、  
そのエンドポイント（例: `http://localhost:50021`）を `.env` ファイルの `AGENT_1_TTS_BASE_URL` に指定してください。

### 🎧 **音声認識(STT)**

| Service | Support Status |
| - | - |
| faster-whisper | 🟢 対応済 |
| OpenAI Whisper | ❌ 未対応(将来対応予定) |

`faster-whisper`はPythonライブラリを通じて動作しますが、特別な外部サービス起動は不要です。（`requirements.txt`をインストールすることで利用可能になります）

### **エンドポイント** 

| Endpoint        | Support Status             |
|-----------------|----------------------------|
| text to text    | 🟢 対応済                  |
| text to voice   | 🟢 対応済                  |
| text to video   | ❌ 未対応(将来対応予定)   |
| voice to text   | 🟢 対応済                  |
| voice to voice  | 🟢 対応済                  |
| voice to video  | ❌ 未対応(将来対応予定)   |
| video to text   | ❌ 未対応(将来対応予定)   |
| video to voice  | ❌ 未対応(将来対応予定)   |
| video to video  | ❌ 未対応(将来対応予定)   |

### **サービス統合**

| Service | Support Status             |
|---------|----------------------------|
| LINE    | 🟢 対応済                  |
| Slack   | ❌ 未対応(将来対応予定)   |
| Discord | ❌ 未対応(将来対応予定)   |

※ 未対応機能・サービスについては、[Projectタブ](https://github.com/0235-jp/karakuri_agent/projects)や[Discussions](https://github.com/0235-jp/karakuri_agent/discussions)で開発状況・ロードマップをご確認ください。

## ⚡ 特徴

- **幅広いエンドポイント対応**  
  - 📝 Text to Text  
  - 💬 Text to Voice  
  - 🎤 Voice to Text 
  - 🔄 Voice to Voice

- **モデル選択の柔軟性**  
  LiteLLMを用いているため、LiteLLMがサポートしているモデル (例: OpenAI, Ollama) に対応可能。

- **複数エージェント管理**  
  `.env` で複数エージェントを定義可能。役割や性格、音声プロファイルなどを自由に設定できます。

- **サービス連携**  
  現在は **LINE** に対応。将来的には他のメッセージングサービスや音声インターフェースとの連携も予定。

## 🎥 デモ・スクリーンショット

*開発中のため、後日更新予定！*  
Flutterクライアントの画面や音声インタラクションのGIFを公開予定です。

## 📦 環境要件

- **サーバーサイド**  
  - Python 3.8以上  
  - (任意) VoicevoxEngine (TTS用)  
    - ローカルでVoicevoxEngineを起動し、`.env`でそのURLを設定してください  
  - (任意) LINE連携を利用する場合は、[LINE Developer Console](https://developers.line.biz/ja/)よりトークン・シークレット取得が必要

- **各種LLMサービスモデル利用時**  
  - 有効なAPIキーおよびインターネット接続が必要。

- **Ollamaモデル利用時**  
  - Ollamaをローカルで起動する必要があります。

## 🛠️ インストール方法

### Docker Composeを利用する場合

プロジェクトルートにある`compose.yml`を使用すると、  
サーバーをコンテナで一括起動できます。

```bash
docker compose up
```

上記コマンドで`http://localhost:8080` でサーバーが起動します。

`.env`は事前に設定しておいてください。

### サーバーサイドのセットアップ(手動)

1. `.env`ファイルの作成、及び必要に応じて修正
   ```bash
   cp example.env .env
   ```
   `.env`でエージェント数やモデル設定、APIキーなどを設定します。
   
2. 必要なパッケージインストール  
   ```bash
   pip install -r requirements.txt
   ```
3. サーバー起動  
   ```bash
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8080
   ```
   → `http://localhost:8080` でFastAPIサーバーが起動します。

## 💡 使い方

- Swagger UIでAPIのフォーマットを確認および実行  
  `http://localhost:8080/docs`

## ⚙️ 設定・カスタマイズ

`.env` や環境変数で設定可能です。例:  
```
API_KEYS=サーバーアクセス用のAPIキーをカンマ区切りで指定
LINE_MAX_AUDIO_FILES=LINE統合で利用する音声ファイルの最大保存件数 (例:5)
LINE_AUDIO_FILES_DIR=line_audio_files
CHAT_MAX_AUDIO_FILES=Chatエンドポイントで利用する音声ファイルの最大保存件数 (例:5)
CHAT_AUDIO_FILES_DIR=chat_audio_files
WEB_SOCKET_MAX_AUDIO_FILES=WebSocketエンドポイントで利用する音声ファイルの最大保存件数 (例:5)
WEB_SOCKET_AUDIO_FILES_DIR=websocket_audio_files

AGENT_1_NAME=エージェントの名前
AGENT_1_MESSAGE_GENERATE_LLM_BASE_URL=メッセージ生成用LLMのURL(LiteLLM形式)
AGENT_1_MESSAGE_GENERATE_LLM_API_KEY=メッセージ生成用LLMのAPIキー
AGENT_1_MESSAGE_GENERATE_LLM_MODEL=メッセージ生成用LLMのモデル(LiteLLM形式)
AGENT_1_EMOTION_GENERATE_LLM_BASE_URL=表情生成用LLMのURL(LiteLLM形式)
AGENT_1_EMOTION_GENERATE_LLM_API_KEY=表情生成用LLMのAPIキー
AGENT_1_EMOTION_GENERATE_LLM_MODEL=表情生成用LLMのモデル(LiteLLM形式)
AGENT_1_VISION_GENERATE_LLM_BASE_URL=画像認識用LLMのURL(LiteLLM形式)※メッセージ生成用LLMが画像に非対応な場合に利用
AGENT_1_VISION_GENERATE_LLM_API_KEY=画像認識用LLMのAPIキー
AGENT_1_VISION_GENERATE_LLM_MODEL=画像認識用LLMのモデル(LiteLLM形式)
AGENT_1_TTS_TYPE=利用するTTSのタイプ(対応Type:voicevox,nijivoice(AivisSpeechもvoicevoxを指定ください。))
AGENT_1_TTS_BASE_URL=TTSエンドポイントのURL
AGENT_1_TTS_API_KEY=TTSのAPIキー(なければ空白で問題有りません)
AGENT_1_TTS_SPEAKER_MODEL=TTSのモデル(対応モデル:default)
AGENT_1_TTS_SPEAKER_ID=TTSのスピーカーID
AGENT_1_LLM_SYSTEM_PROMPT=エージェントに設定するシステムプロンプト
AGENT_1_LINE_CHANNEL_SECRET=LINEチャンネルシークレット(LINE統合を利用する場合は必要)
AGENT_1_LINE_CHANNEL_ACCESS_TOKEN=LINEチャンネルアクセストークン(LINE統合を利用する場合は必要)

# AGENT_2_... と接頭辞を増やすことで、複数エージェントが定義可能
```

## 🤝 コントリビュート方法

1. Issueでバグ報告・改善提案を歓迎します！  
2. `main`ブランチから作業用ブランチを作成し、Pull Requestをお送りください。  
3. 機能提案や質問は[Discussions](https://github.com/0235-jp/karakuri_agent/discussions)でも受け付けています。

## 📜 ライセンスについて

本プロジェクトは独自のライセンス規約に基づいています。

- **非商用利用**: 個人による学習・研究・趣味目的などの非商用利用は無料で可能です。改変物を再配布する場合は、著作権表示と本ライセンス全文を保持してください。
- **商用利用**: 本ソフトウェアまたはその改変物を用いて収益を得る、商業的目的での利用には、事前に株式会社0235との商用ライセンス契約が必要です。
不明な点や商用利用のライセンス取得については、[karakuri-agent-support@0235.co.jp](mailto:karakuri-agent-support@0235.co.jp) までお問い合わせください。

詳細は [LICENSE](LICENSE_JP) ファイルをご確認ください。

## 🛠️ 構築・運用サポート

本プロジェクトの構築・運用に関する有償でのサポートも承ります。料金は要件に応じてお見積りとなりますので、詳細は [karakuri-agent-support@0235.co.jp](mailto:karakuri-agent-support@0235.co.jp) までお問い合わせください。

## 🔗 関連プロジェクト・参考資料

- [FastAPI公式ドキュメント](https://fastapi.tiangolo.com/)  
- [Flutter公式ドキュメント](https://docs.flutter.dev/)  
- [LINE Messaging API](https://developers.line.biz/ja/docs/messaging-api/)  
- [LiteLLM](https://github.com/BerriAI/litellm)  
- [Voicevox Engine](https://github.com/VOICEVOX/voicevox_engine)  
- [AivisSpeech Engine](https://github.com/Aivis-Project/AivisSpeech-Engine)  
- [Style-Bert-VITS2](https://github.com/litagin02/Style-Bert-VITS2)  
- [faster-whisper](https://github.com/guillaumekln/faster-whisper)  
- [OpenAI Whisper](https://github.com/openai/whisper)  
- [Ollama](https://docs.ollama.ai/)
- [にじボイスAPIドキュメント](https://docs.nijivoice.com/docs/getting-started)

================
File: README.md
================
# 🤖 Karakuri Agent

日本語版はこちら [README_JP.md](README_JP.md).

**Karakuri Agent** is an open-source project aiming to create an **AI agent** accessible from any environment—whether it's a smart speaker, chat tool, or web application. By integrating various endpoints and services, it aims to realize a world where you can **access a single agent** from **anywhere**.  
You can also define multiple agents simultaneously, each with unique roles, personalities, voices, and models.

## Technical Overview

### 🚀 **Server-Side**
- Framework: FastAPI

### 🪶 **Model Support (LLM)**
By utilizing LiteLLM, you can access models supported by [LiteLLM](https://github.com/BerriAI/litellm), such as OpenAI and Ollama.  
- **Example for OpenAI models**: Obtain an [OpenAI API Key](https://platform.openai.com/) and configure it in `.env`.  
- **Example for Ollama models**: Run [Ollama](https://docs.ollama.ai/) locally and set its URL in `.env`.

### 🎙️ **Text-To-Speech (TTS)**

| Service            | Support Status        |
|--------------------|-----------------------|
| Voicevox Engine     | 🟢 Supported          |
| AivisSpeech Engine  | 🟢 Supported          |
| Niji Voice API      | 🟢 Supported          |
| OpenAI              | ❌ Not supported yet (planned) |
| Style-Bert-VITS2    | ❌ Not supported yet (planned) |

**Example for VoicevoxEngine Setup**:  
VoicevoxEngine must be running locally.  
Follow the [official documentation](https://github.com/VOICEVOX/voicevox_engine) to start it.  
Then set the endpoint (e.g., `http://localhost:50021`) in `AGENT_1_TTS_BASE_URL` within your `.env` file.

### 🎧 **Speech-To-Text (STT)**

| Service         | Support Status        |
|-----------------|-----------------------|
| faster-whisper  | 🟢 Supported          |
| OpenAI Whisper  | ❌ Not supported yet (planned) |

`faster-whisper` works as a Python library and does not require an external service.  
Once installed via `requirements.txt`, it's ready to use.

### **Endpoints**

| Endpoint        | Support Status             |
|-----------------|----------------------------|
| text to text    | 🟢 Supported               |
| text to voice   | 🟢 Supported               |
| text to video   | ❌ Not supported yet (planned) |
| voice to text   | 🟢 Supported               |
| voice to voice  | 🟢 Supported               |
| voice to video  | ❌ Not supported yet (planned) |
| video to text   | ❌ Not supported yet (planned) |
| video to voice  | ❌ Not supported yet (planned) |
| video to video  | ❌ Not supported yet (planned) |

### **Service Integrations**

| Service  | Support Status             |
|----------|----------------------------|
| LINE     | 🟢 Supported               |
| Slack    | ❌ Not supported yet (planned) |
| Discord  | ❌ Not supported yet (planned) |

For unsupported features or services, please check the [Project tab](https://github.com/0235-jp/karakuri_agent/projects) or [Discussions](https://github.com/0235-jp/karakuri_agent/discussions) for updates and roadmap.

## ⚡ Features

- **Wide Range of Endpoints**  
  - 📝 Text to Text  
  - 💬 Text to Voice  
  - 🎤 Voice to Text 
  - 🔄 Voice to Voice

- **Flexible Model Selection**  
  By using LiteLLM, you can support any models that LiteLLM supports (e.g., OpenAI, Ollama).

- **Multiple Agent Management**  
  You can define multiple agents in `.env`, customizing roles, personalities, voice profiles, and more.

- **Service Integration**  
  Currently integrates with **LINE**. Future plans include other messaging services and voice interfaces.

## 🎥 Demo / Screenshots

*Under development—will be added later!*  
We plan to provide screenshots or GIFs showing the Flutter client and voice interaction.

## 📦 Requirements

- **Server-Side**  
  - Python 3.8 or later  
  - (Optional) VoicevoxEngine for TTS  
    - Run VoicevoxEngine locally and set the URL in `.env`.  
  - (Optional) For LINE integration, obtain tokens and a secret from the [LINE Developer Console](https://developers.line.biz/en/).

- **LLM Model Usage**  
  - Requires valid API keys and an internet connection.

- **Ollama Model Usage**  
  - Requires Ollama running locally.

## 🛠️ Installation

### Using Docker Compose

If you have `compose.yml` at the project root, you can start the server with Docker Compose.

```bash
docker compose up
```
The above command will start the server at `http://localhost:8080`.

Make sure to set up `.env` beforehand.

### Manual Setup: Server-Side

1. Copy and edit `.env` as needed:  
   ```bash
   cp example.env .env
   ```
   Configure agents, models, API keys, etc. in `.env`.
   
2. Install required packages:  
   ```bash
   pip install -r requirements.txt
   ```
3. Start the server:  
   ```bash
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8080
   ```
   → Server will run at `http://localhost:8080`.

## 💡 Usage

- Check and interact with the API format via Swagger UI:  
  `http://localhost:8080/docs`

## ⚙️ Configuration

You can configure the application via `.env` or environment variables. Example:  
```
API_KEYS=Specify server access API keys separated by commas
LINE_MAX_AUDIO_FILES=Max number of audio files for LINE integration (e.g., 5)
LINE_AUDIO_FILES_DIR=line_audio_files
CHAT_MAX_AUDIO_FILES=Max number of audio files for Chat endpoint (e.g., 5)
CHAT_AUDIO_FILES_DIR=chat_audio_files
WEB_SOCKET_MAX_AUDIO_FILES=Max number of audio files for web_socket endpoint (e.g., 5)
WEB_SOCKET_AUDIO_FILES_DIR=web_socket_audio_files

AGENT_1_NAME=Name of the agent
AGENT_1_MESSAGE_GENERATE_LLM_BASE_URL=Base URL for message generation LLM (LiteLLM style)
AGENT_1_MESSAGE_GENERATE_LLM_API_KEY=API key for message generation LLM
AGENT_1_MESSAGE_GENERATE_LLM_MODEL=Model for message generation LLM (LiteLLM style)
AGENT_1_ANALYZE_GENERATE_LLM_BASE_URL=Base URL for analyze generation LLM (LiteLLM style)
AGENT_1_ANALYZE_GENERATE_LLM_API_KEY=API key for analyze generation LLM
AGENT_1_ANALYZE_GENERATE_LLM_MODEL=Model for analyze generation LLM (LiteLLM style)
AGENT_1_VISION_GENERATE_LLM_BASE_URL=Base URL for vision LLM (LiteLLM style)※Used when LLM for message generation does not support images
AGENT_1_VISION_GENERATE_LLM_API_KEY=API key for vision LLM
AGENT_1_VISION_GENERATE_LLM_MODEL=Model for vision LLM (LiteLLM style)
AGENT_1_TTS_TYPE=TTS type (supported: voicevox,nijivoice; use voicevox for AivisSpeech as well)
AGENT_1_TTS_BASE_URL=TTS endpoint URL
AGENT_1_TTS_API_KEY=TTS API key (can be blank)
AGENT_1_TTS_SPEAKER_MODEL=TTS model (e.g., default)
AGENT_1_TTS_SPEAKER_ID=TTS speaker ID
AGENT_1_LLM_SYSTEM_PROMPT=System prompt for the agent
AGENT_1_LINE_CHANNEL_SECRET=LINE channel secret (required if LINE integration is used)
AGENT_1_LINE_CHANNEL_ACCESS_TOKEN=LINE channel access token (required if LINE integration is used)

# Increase AGENT_2_, AGENT_3_, etc. for multiple agents
```

## 🤝 Contributing

1. Feel free to file issues for bugs or improvement suggestions!  
2. Create a working branch from `main` and open a Pull Request.  
3. Feature proposals and questions are also welcome in [Discussions](https://github.com/0235-jp/karakuri_agent/discussions).

## 📜 About the License

This project is provided under a custom license agreement.

- **Non-Commercial Use**: Non-commercial use, such as personal learning, research, or hobby projects, is permitted free of charge. If you redistribute modified versions, please retain the original copyright notice and the full text of the license.
  
- **Commercial Use**: If you wish to use this software or its derivatives for commercial purposes or to generate revenue, you must obtain a commercial license from 0235 Inc. in advance.  
  If you have any questions or need to acquire a commercial license, please contact us at [karakuri-agent-support@0235.co.jp](mailto:karakuri-agent-support@0235.co.jp).

For more details, please see the [LICENSE](LICENSE) file.

## 🛠️ Support for setup, configuration, and ongoing operation

We also offer paid support for setup, configuration, and ongoing operation. Fees depend on your requirements. For more information, please contact us at [karakuri-agent-support@0235.co.jp](mailto:karakuri-agent-support@0235.co.jp)

## 🔗 Related Projects / References

- [FastAPI Documentation](https://fastapi.tiangolo.com/)  
- [Flutter Documentation](https://docs.flutter.dev/)  
- [LINE Messaging API](https://developers.line.biz/en/docs/messaging-api/)  
- [LiteLLM](https://github.com/BerriAI/litellm)  
- [Voicevox Engine](https://github.com/VOICEVOX/voicevox_engine)  
- [AivisSpeech Engine](https://github.com/Aivis-Project/AivisSpeech-Engine)  
- [Style-Bert-VITS2](https://github.com/litagin02/Style-Bert-VITS2)  
- [faster-whisper](https://github.com/guillaumekln/faster-whisper)  
- [OpenAI Whisper](https://github.com/openai/whisper)  
- [Ollama](https://docs.ollama.ai/)
- [Niji voice API Documentation](https://docs.nijivoice.com/docs/getting-started)

================
File: requirements.txt
================
fastapi==0.115.6
uvicorn[standard]==0.34.0
python-dotenv==1.0.1
litellm==1.55.12
aiohttp==3.11.11
pydantic==2.10.4
faster-whisper==1.1.0
soundfile==0.12.1
python-multipart==0.0.20
line-bot-sdk==3.14.2
pydub==0.25.1
pyright==1.1.391
ruff==0.8.4
redis==5.2.1
pytz==2024.2

================
File: start.sh
================
#!/bin/sh

exec uvicorn app.main:app --reload --host 0.0.0.0 --port 8080 --log-config /app/uvicorn_log_config.yaml

================
File: uvicorn_log_config.yaml
================
version: 1
disable_existing_loggers: false

formatters:
  standard:
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  access:
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

handlers:
  console:
    class: logging.StreamHandler
    formatter: standard
    stream: ext://sys.stdout
  file:
    class: logging.handlers.RotatingFileHandler
    formatter: standard
    filename: /app/logs/application.log
    maxBytes: 10485760  # 10MB
    backupCount: 5
  access_file:
    class: logging.handlers.RotatingFileHandler
    formatter: access
    filename: /app/logs/access.log
    maxBytes: 10485760  # 10MB
    backupCount: 5

loggers:
  uvicorn:
    handlers: [console, file]
    level: INFO
  uvicorn.access:
    handlers: [access_file]
    level: INFO
    propagate: false
  watchfiles.main: 
    level: WARNING 
    handlers: [console, file]

root:
  level: INFO
  handlers: [console, file]
  propagate: no
